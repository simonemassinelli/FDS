{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PCtJstuwk1t"
      },
      "source": [
        "The core strategy in this notebook is Stacked Generalization (Stacking), a highly effective form of Meta-Ensemble Learning.\n",
        "\n",
        "Base Layer (Level 0) Modeling A diverse set of Base Models (including Ridge Regression, QDA, SVC, Logistic Regression, and various tree-based ensembles like Hist Gradient Boosting, Random Forest, and Extra Trees) are trained. Each base model is trained on either the Timeline or Combat feature view.\n",
        "\n",
        "The key output of this layer is the set of Out-of-Fold (OOF) probability predictions from all base models on the training data.\n",
        "\n",
        "Meta Layer (Level 1) Modeling The OOF probabilities generated by the Base Layer are concatenated to form a new, small feature matrix. This matrix is used to train a Meta-Model (Logistic Regression).\n",
        "\n",
        "The Meta-Model learns how to optimally combine (weigh) the strengths and weaknesses of the base models' predictions to produce a final, improved probability score, which is then converted to the final prediction using an optimized probability threshold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "rAJ-GAyTtyld"
      },
      "outputs": [],
      "source": [
        "import os, json, math, warnings\n",
        "from typing import Dict, Any, List, Tuple\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from joblib import Parallel, delayed\n",
        "import multiprocessing\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from packaging.version import Version\n",
        "from sklearn import __version__ as skl_version\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_classif, VarianceThreshold\n",
        "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV, RidgeClassifierCV\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "sN4efmSZiWLS",
        "outputId": "4a185590-02bf-4d6c-92e1-2c65faf4dd3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Train data loaded: 10000 records\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "# --- percorso assoluto su Kaggle ---\n",
        "DATA_PATH = \"/kaggle/input/fds-pokemon-battles-prediction-2025\"\n",
        "SUBMISSION_PATH = \"submission.csv\"\n",
        "\n",
        "\n",
        "train_file_path = os.path.join(DATA_PATH, \"train.jsonl\")\n",
        "test_file_path = os.path.join(DATA_PATH, \"test.jsonl\")\n",
        "\n",
        "train_data = []\n",
        "with open(train_file_path, \"r\") as f:\n",
        "    for line in f:\n",
        "        train_data.append(json.loads(line))\n",
        "\n",
        "test_data = []\n",
        "with open(test_file_path, \"r\") as f:\n",
        "    for line in f:\n",
        "        test_data.append(json.loads(line))\n",
        "\n",
        "print(f\"✅ Train data loaded: {len(train_data)} records\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "4sJxioTUUhiO"
      },
      "outputs": [],
      "source": [
        "# Set a fixed random seed for reproducibility.\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "bjo13D6TUjT_"
      },
      "outputs": [],
      "source": [
        "# Define a list of turn cutoffs for feature extraction.\n",
        "TURN_CUTOFFS = [None]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "4PWmee_wUu8l"
      },
      "outputs": [],
      "source": [
        "# Load data from a JSONL file, parsing each line as a JSON object.\n",
        "def load_jsonl(path: str) -> List[Dict[str, Any]]:\n",
        "    rows = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                rows.append(json.loads(line))\n",
        "    return rows\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "r9SYh-lfUy_9"
      },
      "outputs": [],
      "source": [
        "# Define type effectiveness multipliers for Generation 1 Pokémon battles.\n",
        "GEN1_TYPES = {\n",
        "    \"normal\": {\"rock\": 0.5, \"ghost\": 0.0, \"steel\": 0.5},\n",
        "    \"fire\": {\"fire\": 0.5, \"water\": 0.5, \"grass\": 2.0, \"ice\": 2.0, \"bug\": 2.0, \"rock\": 0.5, \"dragon\": 0.5, \"steel\": 2.0},\n",
        "    \"water\": {\"fire\": 2.0, \"water\": 0.5, \"grass\": 0.5, \"ground\": 2.0, \"rock\": 2.0, \"dragon\": 0.5},\n",
        "    \"electric\": {\"water\": 2.0, \"electric\": 0.5, \"grass\": 0.5, \"ground\": 0.0, \"flying\": 2.0, \"dragon\": 0.5},\n",
        "    \"grass\": {\"fire\": 0.5, \"water\": 2.0, \"grass\": 0.5, \"poison\": 0.5, \"ground\": 2.0, \"flying\": 0.5, \"bug\": 0.5, \"rock\": 2.0, \"dragon\": 0.5, \"steel\": 0.5},\n",
        "    \"ice\": {\"fire\": 0.5, \"water\": 0.5, \"grass\": 2.0, \"ground\": 2.0, \"flying\": 2.0, \"dragon\": 2.0, \"steel\": 0.5},\n",
        "    \"fighting\": {\"normal\": 2.0, \"ice\": 2.0, \"poison\": 0.5, \"flying\": 0.5, \"psychic\": 0.5, \"bug\": 0.5, \"rock\": 2.0, \"ghost\": 0.0, \"dark\": 2.0, \"steel\": 2.0, \"fairy\": 0.5},\n",
        "    \"poison\": {\"grass\": 2.0, \"poison\": 0.5, \"ground\": 0.5, \"rock\": 0.5, \"ghost\": 0.5, \"steel\": 0.0, \"fairy\": 2.0},\n",
        "    \"ground\": {\"fire\": 2.0, \"electric\": 2.0, \"grass\": 0.5, \"poison\": 2.0, \"flying\": 0.0, \"bug\": 0.5, \"rock\": 2.0, \"steel\": 2.0},\n",
        "    \"flying\": {\"electric\": 0.5, \"grass\": 2.0, \"fighting\": 2.0, \"bug\": 2.0, \"rock\": 0.5, \"steel\": 0.5},\n",
        "    \"psychic\": {\"fighting\": 2.0, \"poison\": 2.0, \"psychic\": 0.5, \"dark\": 0.0, \"steel\": 0.5},\n",
        "    \"bug\": {\"fire\": 0.5, \"grass\": 2.0, \"fighting\": 0.5, \"poison\": 0.5, \"flying\": 0.5, \"psychic\": 2.0, \"ghost\": 0.5, \"dark\": 2.0, \"steel\": 0.5, \"fairy\": 0.5},\n",
        "    \"rock\": {\"fire\": 2.0, \"ice\": 2.0, \"fighting\": 0.5, \"ground\": 0.5, \"flying\": 2.0, \"bug\": 2.0, \"steel\": 0.5},\n",
        "    \"ghost\": {\"normal\": 0.0, \"psychic\": 2.0, \"ghost\": 2.0, \"dark\": 0.5},\n",
        "    \"dragon\": {\"dragon\": 2.0, \"steel\": 0.5, \"fairy\": 0.0},\n",
        "    \"dark\": {\"fighting\": 0.5, \"psychic\": 2.0, \"ghost\": 2.0, \"dark\": 0.5, \"fairy\": 0.5},\n",
        "    \"steel\": {\"fire\": 0.5, \"water\": 0.5, \"electric\": 0.5, \"ice\": 2.0, \"rock\": 2.0, \"steel\": 0.5, \"fairy\": 2.0},\n",
        "    \"fairy\": {\"fire\": 0.5, \"fighting\": 2.0, \"poison\": 0.5, \"dragon\": 2.0, \"dark\": 2.0, \"steel\": 0.5}}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "k3MjEZX135KL"
      },
      "outputs": [],
      "source": [
        "# Remove a specific battle entry from the training data due to data anomaly.\n",
        "train_data.remove(train_data[4877])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1fNlLI-wnui",
        "outputId": "da523011-fff9-4e00-c10b-d34463a31038"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "55\n",
            "55\n",
            "55\n",
            "55\n",
            "55\n",
            "55\n",
            "55\n",
            "55\n",
            "55\n",
            "55\n",
            "55\n",
            "55\n",
            "85\n",
            "9986\n"
          ]
        }
      ],
      "source": [
        "# Filter out training battles where any player 1 Pokémon's level is not 100.\n",
        "# This ensures consistency in competitive battle assumptions.\n",
        "for i, battle in enumerate(train_data):\n",
        "  squadra1 = battle['p1_team_details']\n",
        "  for pok in squadra1:\n",
        "    livello = pok['level']\n",
        "    if livello < 100:\n",
        "      print(livello)\n",
        "      train_data.remove(train_data[i])\n",
        "\n",
        "print(len(train_data))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "xkN7scKhU0gL"
      },
      "outputs": [],
      "source": [
        "# Calculate the type effectiveness multiplier for an attack against defending Pokémon types.\n",
        "def type_multiplier(attack_type: str, defend_types: List[str]) -> float:\n",
        "    at = str(attack_type or \"\").lower()\n",
        "    mult = 1.0\n",
        "    chart = GEN1_TYPES.get(at, {})\n",
        "    for dt in defend_types or []:\n",
        "        mult *= chart.get(str(dt or \"\").lower(), 1.0) # Multiply for each defending type\n",
        "    return mult\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "xEbazh2PU11L"
      },
      "outputs": [],
      "source": [
        "# Safely convert a value to a float, returning a default if conversion fails.\n",
        "def safe_float(x, default=1.0):\n",
        "    try:\n",
        "        return float(x)\n",
        "    except:\n",
        "        return default\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "OtNqTdUsU3EH"
      },
      "outputs": [],
      "source": [
        "# Identify all Pokémon species seen on the opponent's side in a battle timeline.\n",
        "def species_seen_in_timeline(timeline: List[Dict[str, Any]]) -> set:\n",
        "    names = set()\n",
        "    for turn in timeline or []:\n",
        "        name = (turn.get(\"p2_pokemon_state\") or {}).get(\"name\")\n",
        "        if name:\n",
        "            names.add(str(name).lower())\n",
        "    return names\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "a8EXzpxMU4Hf"
      },
      "outputs": [],
      "source": [
        "# Discover all unique Pokémon types present in the provided battle data.\n",
        "def discover_all_types(data: List[Dict[str, Any]]) -> List[str]:\n",
        "    s = set()\n",
        "    for b in data:\n",
        "        for p in b.get(\"p1_team_details\") or []:\n",
        "            for t in p.get(\"types\") or []:\n",
        "                tl = str(t or \"\").lower()\n",
        "                if tl and tl != \"notype\" and tl in GEN1_TYPES:\n",
        "                    s.add(tl)\n",
        "    return sorted(s)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "AXzwCrnEU5Ka"
      },
      "outputs": [],
      "source": [
        "# Identify and store all unique Pokémon types from the training data.\n",
        "ALL_TYPES = discover_all_types(train_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIesUETAU7UQ",
        "outputId": "b51f97b3-4518-43b4-aa9f-a5cd286e8d01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['dragon', 'electric', 'fire', 'flying', 'ghost', 'grass', 'ground', 'ice', 'normal', 'poison', 'psychic', 'rock', 'water']\n",
            "13\n"
          ]
        }
      ],
      "source": [
        "# Print the discovered Pokémon types and their count.\n",
        "print(ALL_TYPES)\n",
        "print(len(ALL_TYPES))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7xjPu1m9eT2"
      },
      "source": [
        "Identify top species and moves to track as features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "YpdAJkszU-My"
      },
      "outputs": [],
      "source": [
        "# Identify the most common Pokémon species appearing in battles,\n",
        "# considering both player 1's team and Pokémon seen in the timeline.\n",
        "def top_species(data: List[Dict[str, Any]], top_k=120) -> List[str]:\n",
        "    c = Counter()\n",
        "    for b in data:\n",
        "        for p in b.get(\"p1_team_details\") or []:\n",
        "            nm = str(p.get(\"name\",\"\")).lower()\n",
        "            if nm: c[nm]+=1\n",
        "        for sp in species_seen_in_timeline(b.get(\"battle_timeline\")):\n",
        "            c[sp]+=0.5 # Give less weight to seen species than on-team species\n",
        "    return [s for s,_ in c.most_common(top_k)]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "crohto8SVEei"
      },
      "outputs": [],
      "source": [
        "# Determine the top K most frequently appearing Pokémon species.\n",
        "TOP_SPECIES = top_species(train_data, 120)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNgErqdQVG-2",
        "outputId": "285f505e-c021-42d4-c416-4318bf3e8f1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['chansey', 'tauros', 'snorlax', 'exeggutor', 'alakazam']\n"
          ]
        }
      ],
      "source": [
        "# Print the first few top species identified.\n",
        "print(TOP_SPECIES[:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "Y-h-8yAHVL3k"
      },
      "outputs": [],
      "source": [
        "# Build a mapping from Pokémon names to their types based on training data.\n",
        "NAME_TO_TYPES: Dict[str, List[str]] = {}\n",
        "for b in train_data:\n",
        "    for p in b.get(\"p1_team_details\") or []:\n",
        "        nm = str(p.get(\"name\",\"\")).lower()\n",
        "        if nm and nm not in NAME_TO_TYPES:\n",
        "            NAME_TO_TYPES[nm] = [t.lower() for t in (p.get(\"types\") or []) if t and t!=\"notype\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "6Kgu_-wpVPgI"
      },
      "outputs": [],
      "source": [
        "# Calculate the STAB (Same-Type Attack Bonus) adjusted base power of a move.\n",
        "def stab_adjusted_bp(move, attacker_name: str) -> float:\n",
        "    if not move:\n",
        "      return 0.0\n",
        "    bp = float(move.get(\"base_power\") or 0)\n",
        "    if bp <= 0:\n",
        "      return 0.0\n",
        "    mtype = str(move.get(\"type\") or \"\").lower()\n",
        "    if not mtype or not attacker_name:\n",
        "      return bp\n",
        "    attacker_types = NAME_TO_TYPES.get(attacker_name, [])\n",
        "    # Apply STAB if move type matches attacker's type\n",
        "    return bp * 1.5 if mtype in attacker_types else bp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "3nrh-XWgVZlq"
      },
      "outputs": [],
      "source": [
        "# Build prior probabilities of opponent Pokémon appearing in the timeline given their lead Pokémon.\n",
        "def build_lead_back_priors(train_data, top_species):\n",
        "    co = defaultdict(lambda: Counter()) # Count co-occurrences of lead and seen species\n",
        "    lead_cnt = Counter() # Count occurrences of each lead species\n",
        "    for b in train_data:\n",
        "        lead = (b.get(\"p2_lead_details\") or {}).get(\"name\",\"\")\n",
        "        lead = str(lead).lower()\n",
        "        if not lead: continue\n",
        "        seen = species_seen_in_timeline(b.get(\"battle_timeline\"))\n",
        "        lead_cnt[lead]+=1\n",
        "        for s in seen:\n",
        "            co[lead][s]+=1\n",
        "    priors = {}\n",
        "    for lead, cnt in lead_cnt.items():\n",
        "        priors[lead] = {s: co[lead][s]/cnt for s in top_species} # Calculate probability\n",
        "    return priors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "FutEchmXVbOF"
      },
      "outputs": [],
      "source": [
        "# Calculate prior probabilities of opponent Pokémon appearing after a specific lead.\n",
        "P_LEAD_BACK = build_lead_back_priors(train_data, TOP_SPECIES)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "q3_aS3WRVdOj"
      },
      "outputs": [],
      "source": [
        "# Generate features representing prior probabilities of opponent Pokémon appearing given their lead.\n",
        "def prior_features_for_lead(lead_name: str) -> Dict[str,float]:\n",
        "    lead = str(lead_name or \"\").lower()\n",
        "    row = P_LEAD_BACK.get(lead, {}) # Get prior probabilities for the given lead\n",
        "    feats = {f\"p2_prob_{s}_given_lead\": float(row.get(s, 0.0)) for s in TOP_SPECIES}\n",
        "    return feats\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "TqQJTOLqWbVk"
      },
      "outputs": [],
      "source": [
        "# Normalize a move name by converting to lowercase and removing spaces/hyphens.\n",
        "def normalize_move_name(name: str) -> str:\n",
        "    if not name:\n",
        "        return \"\"\n",
        "    n = str(name).strip().lower()\n",
        "    n = n.replace(\" \", \"\").replace(\"-\", \"\")\n",
        "    return n\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "IBo7A8RnVgFy"
      },
      "outputs": [],
      "source": [
        "# Count the occurrences of each move in the battle timelines for both players.\n",
        "def move_counts(battles):\n",
        "    c = Counter()\n",
        "    for b in battles:\n",
        "        for tr in b.get(\"battle_timeline\") or []:\n",
        "            for side in (\"p1\",\"p2\"):\n",
        "                md = tr.get(f\"{side}_move_details\")\n",
        "                if md and md.get(\"name\"):\n",
        "                    c[normalize_move_name(md[\"name\"])] += 1\n",
        "    return c\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtyWDAj1Vl3D",
        "outputId": "3f420645-948b-4054-e47d-8135469d9a95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRACK_MOVES size: 33\n"
          ]
        }
      ],
      "source": [
        "# Build a vocabulary of most frequently used moves to cover a target percentage of all moves.\n",
        "def build_move_vocab_by_coverage(battles, target_coverage=0.985, min_freq=3):\n",
        "    c = move_counts(battles)\n",
        "    total = sum(c.values()) if c else 1\n",
        "    running = 0\n",
        "    vocab = []\n",
        "    for m,f in c.most_common():\n",
        "        if f < min_freq:\n",
        "            break\n",
        "        vocab.append(m)\n",
        "        running += f\n",
        "        if running / total >= target_coverage:\n",
        "            break\n",
        "    return vocab, c\n",
        "\n",
        "TRACK_MOVES, MOVE_FREQ = build_move_vocab_by_coverage(train_data, target_coverage=0.985, min_freq=3)\n",
        "\n",
        "print(\"TRACK_MOVES size:\", len(TRACK_MOVES))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLouoDIaWpF7",
        "outputId": "7f59a567-443c-4e8a-885e-8289e7169d32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['bodyslam', 'psychic', 'thunderwave', 'blizzard', 'thunderbolt', 'icebeam', 'seismictoss', 'earthquake', 'sleeppowder', 'softboiled', 'hyperbeam', 'rest', 'recover', 'reflect', 'lovelykiss', 'clamp', 'amnesia', 'drillpeck', 'surf', 'hypnosis', 'sing', 'stunspore', 'explosion', 'wrap', 'counter', 'doubleedge', 'nightshade', 'rockslide', 'megadrain', 'selfdestruct', 'confuseray', 'substitute', 'doublekick']\n"
          ]
        }
      ],
      "source": [
        "# Print the list of tracked moves, which represent common and impactful moves.\n",
        "print(TRACK_MOVES)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "8_RKPGYdW-Qx"
      },
      "outputs": [],
      "source": [
        "# Define sets of specific move categories.\n",
        "STATUS_MOVES = {\n",
        "    \"thunderwave\",\"stunspore\",\"sleeppowder\",\"sing\",\"lovelykiss\",\"toxic\",\n",
        "    \"hypnosis\",\"substitute\",\"reflect\",\"recover\",\"softboiled\",\"rest\",\n",
        "    \"agility\",\"swordsdance\",\"amnesia\"\n",
        "}\n",
        "PARTIAL_TRAP = {\"wrap\",\"clamp\",\"firespin\"}\n",
        "EXPLODE     = {\"explosion\",\"selfdestruct\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "aTQEosZ85Yla"
      },
      "outputs": [],
      "source": [
        "# Define sets for healing and priority moves.\n",
        "HEALING_MOVES = {\n",
        "    \"recover\",\"softboiled\",\"rest\",\n",
        "    \"megadrain\"\n",
        "}\n",
        "PRIORITY_MOVES = {}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "2UB0XZXhf7oQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Collect all unique move names observed across all battle timelines.\n",
        "def collect_all_moves(battles):\n",
        "    moves = set()\n",
        "    for b in battles:\n",
        "        for tr in b.get(\"battle_timeline\") or []:\n",
        "            for side in (\"p1\", \"p2\"):\n",
        "                md = tr.get(f\"{side}_move_details\")\n",
        "                if md and md.get(\"name\"):\n",
        "                    mv = normalize_move_name(md[\"name\"])\n",
        "                    if mv:\n",
        "                        moves.add(mv)\n",
        "    return moves\n",
        "\n",
        "ALL_MOVES = collect_all_moves(train_data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "aSbcgZO5jRO5"
      },
      "outputs": [],
      "source": [
        "# Dynamically identify status and explosive moves from battle timelines.\n",
        "# This helps capture moves not explicitly listed in static sets.\n",
        "def dynamic_status_moves(battles):\n",
        "    s = set()\n",
        "    for b in battles:\n",
        "        for tr in b.get(\"battle_timeline\") or []:\n",
        "            for side in (\"p1\",\"p2\"):\n",
        "                md = tr.get(f\"{side}_move_details\")\n",
        "                if not md: continue\n",
        "                mv = normalize_move_name(md[\"name\"])\n",
        "                cat = md.get(\"category\",\"\").lower()\n",
        "                if cat == \"status\": s.add(mv)\n",
        "                if mv in {\"toxic\",\"thunderwave\"}: s.add(mv)\n",
        "    return s\n",
        "\n",
        "def dynamic_explode(battles):\n",
        "    boom = set()\n",
        "    for b in battles:\n",
        "        for tr in b.get(\"battle_timeline\") or []:\n",
        "            for side in (\"p1\",\"p2\"):\n",
        "                md = tr.get(f\"{side}_move_details\")\n",
        "                if md:\n",
        "                    mv = normalize_move_name(md[\"name\"])\n",
        "                    if \"explode\" in mv or \"selfdestruct\" in mv:\n",
        "                        boom.add(mv)\n",
        "    return boom\n",
        "\n",
        "DYN_STATUS  = dynamic_status_moves(train_data)\n",
        "DYN_EXPLODE = dynamic_explode(train_data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cAcIDThie2j",
        "outputId": "96d027ae-1e56-4239-b266-d77ebfa009c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "STATUS_MOVES: {'sing', 'softboiled', 'amnesia', 'confuseray', 'swordsdance', 'thunderwave', 'agility', 'rest', 'hypnosis', 'sleeppowder', 'reflect', 'substitute', 'recover', 'lovelykiss', 'toxic', 'stunspore'}\n",
            "HEALING_MOVES: {'softboiled', 'recover', 'megadrain', 'rest'}\n",
            "PARTIAL_TRAP: {'firespin', 'wrap', 'clamp'}\n",
            "EXPLODE: {'selfdestruct', 'explosion'}\n",
            "PRIORITY_MOVES: {}\n"
          ]
        }
      ],
      "source": [
        "# Update move category sets with dynamically discovered moves,\n",
        "# filtering for moves present in ALL_MOVES to ensure relevance.\n",
        "STATUS_MOVES   = (STATUS_MOVES | DYN_STATUS) & ALL_MOVES\n",
        "PARTIAL_TRAP   = PARTIAL_TRAP & ALL_MOVES\n",
        "EXPLODE        = (EXPLODE | DYN_EXPLODE) & ALL_MOVES\n",
        "HEALING_MOVES  = HEALING_MOVES & ALL_MOVES\n",
        "\n",
        "\n",
        "print(\"STATUS_MOVES:\", STATUS_MOVES)\n",
        "print(\"HEALING_MOVES:\", HEALING_MOVES)\n",
        "print(\"PARTIAL_TRAP:\", PARTIAL_TRAP)\n",
        "print(\"EXPLODE:\", EXPLODE)\n",
        "print(\"PRIORITY_MOVES:\", PRIORITY_MOVES)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsEhOkM5fFE_",
        "outputId": "038b5019-d8bd-401e-aab0-ff2664ad0b4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16\n",
            "3\n",
            "2\n",
            "4\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "# Print the number of moves in each defined category for summary.\n",
        "print(len(STATUS_MOVES))\n",
        "print(len(PARTIAL_TRAP))\n",
        "print(len(EXPLODE))\n",
        "print(len(HEALING_MOVES))\n",
        "print(len(PRIORITY_MOVES))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "t8vNKZhtXAK2"
      },
      "outputs": [],
      "source": [
        "# Calculate the sum of stat boosts and count move occurrences within a battle timeline.\n",
        "# Also, assess early status effects.\n",
        "def sum_boosts(d):\n",
        "    if not d: return 0.0\n",
        "    s=0.0\n",
        "    for v in d.values():\n",
        "        try: s+=float(v)\n",
        "        except: pass\n",
        "    return s\n",
        "\n",
        "def timeline_move_counts(timeline) -> Dict[str,float]:\n",
        "    feats = {f\"p1_mv_{m}\":0.0 for m in TRACK_MOVES}\n",
        "    feats.update({f\"p2_mv_{m}\":0.0 for m in TRACK_MOVES})\n",
        "    feats[\"p1_mv_other\"] = 0.0 # Track moves not in TRACK_MOVES\n",
        "    feats[\"p2_mv_other\"] = 0.0 # Track moves not in TRACK_MOVES\n",
        "\n",
        "    early_status_score = 0.0\n",
        "    for idx, turn in enumerate(timeline or [], start=1):\n",
        "        for side in (\"p1\",\"p2\"):\n",
        "            md = turn.get(f\"{side}_move_details\")\n",
        "            if md and md.get(\"name\"):\n",
        "                mv = normalize_move_name(md[\"name\"])\n",
        "                key = f\"{side}_mv_{mv}\" if mv in TRACK_MOVES else f\"{side}_mv_other\"\n",
        "                feats[key] += 1.0\n",
        "\n",
        "        # Assess early game status advantage\n",
        "        if idx <= 3:\n",
        "            s1 = (turn.get(\"p1_pokemon_state\") or {}).get(\"status\")\n",
        "            s2 = (turn.get(\"p2_pokemon_state\") or {}).get(\"status\")\n",
        "            if s2 in (\"slp\",\"frz\",\"par\"): early_status_score += 1.0\n",
        "            if s1 in (\"slp\",\"frz\",\"par\"): early_status_score -= 1.0\n",
        "\n",
        "    feats[\"early_status_score\"] = early_status_score\n",
        "    return feats\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "4VlVoZaC6YLX"
      },
      "outputs": [],
      "source": [
        "# Extract various timeline-based features such as move accuracy, healing, priority,\n",
        "# HP changes, and status accumulation, providing a detailed view of battle flow.\n",
        "def timeline_additions(timeline: List[Dict[str, Any]]) -> Dict[str, float]:\n",
        "    feats = {\n",
        "        \"p1_mult_mean\": 1.0, \"p2_mult_mean\": 1.0,\n",
        "        \"p1_move_accuracy\": 0.0, \"p2_move_accuracy\": 0.0,\n",
        "        \"p1_healing_moves\": 0.0, \"p2_healing_moves\": 0.0,\n",
        "        \"p1_priority_moves\": 0.0, \"p2_priority_moves\": 0.0,\n",
        "        \"p1_status_score\": 0.0, \"p2_status_score\": 0.0,\n",
        "        \"p1_life_turn30\": 1.0, \"p2_life_turn30\": 1.0,\n",
        "\n",
        "        \"mult_diff\": 0.0,\n",
        "        \"diff_move_accuracy\": 0.0,\n",
        "        \"diff_healing_moves\": 0.0,\n",
        "        \"priority_diff\": 0.0,\n",
        "        \"diff_status\": 0.0,\n",
        "        \"life_diff_turn30\": 0.0,\n",
        "    }\n",
        "    if not timeline:\n",
        "        return feats\n",
        "\n",
        "    # Initialize HP tracking for accuracy calculation\n",
        "    prev_p1_hp = safe_float((timeline[0].get(\"p1_pokemon_state\") or {}).get(\"hp_pct\", 1.0))\n",
        "    prev_p2_hp = safe_float((timeline[0].get(\"p2_pokemon_state\") or {}).get(\"hp_pct\", 1.0))\n",
        "\n",
        "    p1_acc_hits = p1_acc_attempts = 0\n",
        "    p2_acc_hits = p2_acc_attempts = 0\n",
        "    p1_heal = p2_heal = 0\n",
        "    p1_pri  = p2_pri  = 0\n",
        "\n",
        "    mult_p1, mult_p2 = [], [] # Store type multipliers\n",
        "\n",
        "    def status_weight(s):\n",
        "        return 1.0 if s==\"par\" else 2.0 if s==\"slp\" else 3.0 if s==\"frz\" else 0.0\n",
        "\n",
        "    last_turn_seen = 0\n",
        "\n",
        "    for tr in timeline:\n",
        "        last_turn_seen = int(tr.get(\"turn\", last_turn_seen)) # Track last turn\n",
        "        p1s = tr.get(\"p1_pokemon_state\") or {}\n",
        "        p2s = tr.get(\"p2_pokemon_state\") or {}\n",
        "        m1  = tr.get(\"p1_move_details\") or {}\n",
        "        m2  = tr.get(\"p2_move_details\") or {}\n",
        "\n",
        "        p1_hp = safe_float(p1s.get(\"hp_pct\", prev_p1_hp))\n",
        "        p2_hp = safe_float(p2s.get(\"hp_pct\", prev_p2_hp))\n",
        "\n",
        "        n1 = normalize_move_name(m1.get(\"name\",\"\"))\n",
        "        n2 = normalize_move_name(m2.get(\"name\",\"\"))\n",
        "        if n1 in HEALING_MOVES:  p1_heal += 1\n",
        "        if n2 in HEALING_MOVES:  p2_heal += 1\n",
        "        if n1 in PRIORITY_MOVES: p1_pri  += 1\n",
        "        if n2 in PRIORITY_MOVES: p2_pri  += 1\n",
        "\n",
        "        # Check if a move is damaging (base power > 0)\n",
        "        def damaging(m): return float(m.get(\"base_power\") or 0) > 0\n",
        "        if m1 and damaging(m1):\n",
        "            p1_acc_attempts += 1\n",
        "            if p2_hp < prev_p2_hp - 1e-9: # If opponent HP decreased, it was a hit\n",
        "                p1_acc_hits += 1\n",
        "        if m2 and damaging(m2):\n",
        "            p2_acc_attempts += 1\n",
        "            if p1_hp < prev_p1_hp - 1e-9:\n",
        "                p2_acc_hits += 1\n",
        "\n",
        "        # Calculate type effectiveness multiplier for damaging moves\n",
        "        def push_mult(m, defender_state, bag):\n",
        "            if not damaging(m): return\n",
        "            mt = str(m.get(\"type\",\"\")).lower()\n",
        "            def_types = [t for t in defender_state.get(\"types\", []) if t and t!=\"notype\"]\n",
        "            bag.append(type_multiplier(mt, def_types))\n",
        "        push_mult(m1, p2s, mult_p1)\n",
        "        push_mult(m2, p1s, mult_p2)\n",
        "\n",
        "        # status score accumulation\n",
        "        feats[\"p1_status_score\"] += status_weight(p1s.get(\"status\"))\n",
        "        feats[\"p2_status_score\"] += status_weight(p2s.get(\"status\"))\n",
        "\n",
        "        prev_p1_hp, prev_p2_hp = p1_hp, p2_hp\n",
        "\n",
        "    # Capture HP at a specific turn (e.g., turn 30) for early game assessment\n",
        "    target_turn = min(30, last_turn_seen if last_turn_seen>0 else 30)\n",
        "    p1_t = p2_t = 1.0\n",
        "    for tr in timeline:\n",
        "        if int(tr.get(\"turn\", 0)) <= target_turn:\n",
        "            p1_t = safe_float((tr.get(\"p1_pokemon_state\") or {}).get(\"hp_pct\", p1_t))\n",
        "            p2_t = safe_float((tr.get(\"p2_pokemon_state\") or {}).get(\"hp_pct\", p2_t))\n",
        "    feats[\"p1_life_turn30\"] = float(p1_t)\n",
        "    feats[\"p2_life_turn30\"] = float(p2_t)\n",
        "    feats[\"life_diff_turn30\"] = float(p1_t - p2_t)\n",
        "\n",
        "    # Calculate mean type multiplier and move accuracy\n",
        "    feats[\"p1_mult_mean\"] = float(np.mean(mult_p1)) if mult_p1 else 1.0\n",
        "    feats[\"p2_mult_mean\"] = float(np.mean(mult_p2)) if mult_p2 else 1.0\n",
        "    p1_acc = (p1_acc_hits / p1_acc_attempts) if p1_acc_attempts > 0 else 0.0\n",
        "    p2_acc = (p2_acc_hits / p2_acc_attempts) if p2_acc_attempts > 0 else 0.0\n",
        "    feats[\"p1_move_accuracy\"] = float(p1_acc)\n",
        "    feats[\"p2_move_accuracy\"] = float(p2_acc)\n",
        "    feats[\"p1_healing_moves\"] = float(p1_heal)\n",
        "    feats[\"p2_healing_moves\"] = float(p2_heal)\n",
        "    feats[\"p1_priority_moves\"] = float(p1_pri)\n",
        "    feats[\"p2_priority_moves\"] = float(p2_pri)\n",
        "\n",
        "\n",
        "    # Calculate differences between players for various metrics\n",
        "    feats[\"mult_diff\"] = feats[\"p1_mult_mean\"] - feats[\"p2_mult_mean\"]\n",
        "    feats[\"diff_move_accuracy\"] = feats[\"p1_move_accuracy\"] - feats[\"p2_move_accuracy\"]\n",
        "    feats[\"diff_healing_moves\"] = feats[\"p1_healing_moves\"] - feats[\"p2_healing_moves\"]\n",
        "    feats[\"priority_diff\"] = feats[\"p1_priority_moves\"] - feats[\"p2_priority_moves\"]\n",
        "    feats[\"diff_status\"] = feats[\"p2_status_score\"] - feats[\"p1_status_score\"]\n",
        "    return feats\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "PLrvbcqIXBwd"
      },
      "outputs": [],
      "source": [
        "# Extract features related to move categories like status, partial trap,\n",
        "# explosion, and STAB (Same-Type Attack Bonus) hits for both players.\n",
        "def move_buckets_features(timeline):\n",
        "    b = {\n",
        "        \"p1_status_moves\":0.0,\"p2_status_moves\":0.0,\n",
        "        \"p1_partial_trap\":0.0,\"p2_partial_trap\":0.0,\n",
        "        \"p1_explodes\":0.0,\"p2_explodes\":0.0,\n",
        "        \"p1_stab_hits\":0.0,\"p2_stab_hits\":0.0\n",
        "    }\n",
        "    for tr in (timeline or []):\n",
        "        p1s = (tr.get(\"p1_pokemon_state\") or {})\n",
        "        p2s = (tr.get(\"p2_pokemon_state\") or {})\n",
        "        for side, ms, opps in [(\"p1\",\"p1_move_details\",\"p2_pokemon_state\"), (\"p2\",\"p2_move_details\",\"p1_pokemon_state\")]:\n",
        "            md = tr.get(ms)\n",
        "            if not md: continue\n",
        "            mv = normalize_move_name(md.get(\"name\",\"\"))\n",
        "            if not mv: continue\n",
        "            # Increment counters for specific move categories\n",
        "            if mv in STATUS_MOVES: b[f\"{side}_status_moves\"] += 1.0\n",
        "            if mv in PARTIAL_TRAP: b[f\"{side}_partial_trap\"] += 1.0\n",
        "            if mv in EXPLODE:      b[f\"{side}_explodes\"]     += 1.0\n",
        "            attacker = (tr.get(f\"{side}_pokemon_state\") or {}).get(\"name\",\"\")\n",
        "            if (md.get(\"base_power\",0) or 0) > 0:\n",
        "                atypes = NAME_TO_TYPES.get(str(attacker).lower(), [])\n",
        "                if str(md.get(\"type\",\"\")).lower() in atypes: # Check for STAB\n",
        "                    b[f\"{side}_stab_hits\"] += 1.0\n",
        "    return b\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "qb5NqqcIXDRb"
      },
      "outputs": [],
      "source": [
        "# Count the number of turns each player is affected by paralysis, sleep, or freeze,\n",
        "# and calculate the differences between players.\n",
        "def status_turn_counts(timeline) -> Dict[str,float]:\n",
        "    t = timeline or []\n",
        "    c = {\"p1_par\":0.0,\"p1_slp\":0.0,\"p1_frz\":0.0,\"p2_par\":0.0,\"p2_slp\":0.0,\"p2_frz\":0.0}\n",
        "    for turn in t:\n",
        "        s1 = (turn.get(\"p1_pokemon_state\") or {}).get(\"status\")\n",
        "        s2 = (turn.get(\"p2_pokemon_state\") or {}).get(\"status\")\n",
        "        if s1 == \"par\": c[\"p1_par\"] += 1\n",
        "        if s1 == \"slp\": c[\"p1_slp\"] += 1\n",
        "        if s1 == \"frz\": c[\"p1_frz\"] += 1\n",
        "        if s2 == \"par\": c[\"p2_par\"] += 1\n",
        "        if s2 == \"slp\": c[\"p2_slp\"] += 1\n",
        "        if s2 == \"frz\": c[\"p2_frz\"] += 1\n",
        "    c[\"par_diff\"] = c[\"p2_par\"] - c[\"p1_par\"]\n",
        "    c[\"slp_diff\"] = c[\"p2_slp\"] - c[\"p1_slp\"]\n",
        "    c[\"frz_diff\"] = c[\"p2_frz\"] - c[\"p1_frz\"]\n",
        "    return c\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKJV1-dN-DBC"
      },
      "source": [
        "Status Effects & Critical Hits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "w8syAZLeXEqj"
      },
      "outputs": [],
      "source": [
        "# Extract features related to early-game crowd control (sleep, freeze) infliction.\n",
        "# This identifies who gains status advantage early in the battle.\n",
        "def early_cc_features(timeline, window_size=5):\n",
        "    feats = {\"first_cc_winner\":0.0, \"early_sleep_hits\":0.0, \"early_freeze_hits\":0.0}\n",
        "    first_cc = None # Track who inflicted the first major status condition\n",
        "    for _, turn in enumerate(timeline[:window_size] or [], start=1):\n",
        "        s1 = (turn.get(\"p1_pokemon_state\") or {}).get(\"status\")\n",
        "        s2 = (turn.get(\"p2_pokemon_state\") or {}).get(\"status\")\n",
        "        if first_cc is None:\n",
        "            if s1 in (\"slp\",\"frz\") and s2 not in (\"slp\",\"frz\"): first_cc = -1\n",
        "            if s2 in (\"slp\",\"frz\") and s1 not in (\"slp\",\"frz\"): first_cc = +1\n",
        "        if s2 == \"slp\": feats[\"early_sleep_hits\"] += 1 # Opponent inflicts sleep\n",
        "        if s1 == \"slp\": feats[\"early_sleep_hits\"] -= 1 # Player inflicts sleep (negative for diff)\n",
        "        if s2 == \"frz\": feats[\"early_freeze_hits\"] += 1\n",
        "        if s1 == \"frz\": feats[\"early_freeze_hits\"] -= 1\n",
        "    feats[\"first_cc_winner\"] = float(first_cc or 0)\n",
        "    return feats\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "L1ajQve0XF6w"
      },
      "outputs": [],
      "source": [
        "# Identify if Hyper Beam resulted in a KO for either player within the timeline.\n",
        "def hyper_beam_ko_flags(timeline):\n",
        "    o = {\"p1_hb_ko\":0.0,\"p2_hb_ko\":0.0}\n",
        "    for turn in timeline or []:\n",
        "        m1 = (turn.get(\"p1_move_details\") or {})\n",
        "        m2 = (turn.get(\"p2_move_details\") or {})\n",
        "        # Check if player 1 used Hyper Beam and opponent fainted\n",
        "        if str(m1.get(\"name\",\"\")).lower().replace(\" \",\"\").replace(\"-\",\"\")==\"hyperbeam\" and (turn.get(\"p2_pokemon_state\") or {}).get(\"status\")==\"fnt\":\n",
        "            o[\"p1_hb_ko\"]=1.0\n",
        "        # Check if player 2 used Hyper Beam and player 1 fainted\n",
        "        if str(m2.get(\"name\",\"\")).lower().replace(\" \",\"\").replace(\"-\",\"\")==\"hyperbeam\" and (turn.get(\"p1_pokemon_state\") or {}).get(\"status\")==\"fnt\":\n",
        "            o[\"p2_hb_ko\"]=1.0\n",
        "    return o\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "XdACwgk6XJBf"
      },
      "outputs": [],
      "source": [
        "# Encode player team types and opponent lead types as binary features.\n",
        "def encode_types_block(player_team: List[Dict[str, Any]], opponent_lead: Dict[str, Any]) -> Dict[str,float]:\n",
        "    feats = {}\n",
        "    player_types=[]\n",
        "    for p in (player_team or []):\n",
        "        for t in p.get(\"types\") or []:\n",
        "            tl = str(t or \"\").lower()\n",
        "            if tl and tl in ALL_TYPES:\n",
        "                player_types.append(tl)\n",
        "    # Binary features for player 1 having each type\n",
        "    for t in ALL_TYPES:\n",
        "        feats[f\"p1_has_{t}\"] = float(player_types.count(t))\n",
        "    # Binary features for opponent's lead having each type\n",
        "    opp_types = [str(t or \"\").lower() for t in (opponent_lead or {}).get(\"types\", []) if str(t or \"\").lower() in ALL_TYPES]\n",
        "    for t in ALL_TYPES:\n",
        "        feats[f\"p2_lead_is_{t}\"] = 1.0 if t in opp_types else 0.0\n",
        "    return feats\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "f0a5kub0XKkI"
      },
      "outputs": [],
      "source": [
        "# Summarize base stats for the player's team and compare them to the opponent's lead Pokémon.\n",
        "def stat_summary_block(player_team: List[Dict[str, Any]], opponent_lead: Dict[str, Any]) -> Dict[str,float]:\n",
        "    feats={}\n",
        "    if not player_team: return feats\n",
        "    cols = [\"base_hp\",\"base_atk\",\"base_def\",\"base_spa\",\"base_spd\",\"base_spe\"]\n",
        "    df = pd.DataFrame([{c:(p.get(c,0) or 0) for c in cols} for p in player_team])\n",
        "    lead = {c:(opponent_lead.get(c,0) or 0) for c in cols}\n",
        "    for c in cols:\n",
        "        feats[f\"p1_avg_{c[5:]}\"] = float(df[c].mean()) # Player 1 average stat\n",
        "        feats[f\"lead_diff_{c[5:]}\"] = float((df.iloc[0][c] if len(df)>0 else 0) - lead[c]) # Difference between P1's first pokemon and P2 lead\n",
        "    feats[\"p1_total_avg\"] = float(df.sum(axis=1).mean()) # Player 1 average total stats\n",
        "    feats[\"p2_lead_total\"] = float(sum(lead.values())) # Opponent lead total stats\n",
        "    feats[\"total_diff\"] = feats[\"p1_total_avg\"] - feats[\"p2_lead_total\"]\n",
        "    feats[\"speed_adv\"] = float(df[\"base_spe\"].mean() - lead[\"base_spe\"]) # Player 1 average speed advantage\n",
        "    feats[\"p1_fastest_spe\"] = float(df[\"base_spe\"].max()) # Player 1 fastest pokemon speed\n",
        "    feats[\"p1_bulk_sum95\"] = float(np.percentile(df[\"base_hp\"]+df[\"base_def\"]+df[\"base_spd\"], 95)) # Player 1 95th percentile bulk\n",
        "    return feats\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "YN1ChjnEXL5x"
      },
      "outputs": [],
      "source": [
        "# Extract features related to the type and speed matchup between the player's team and the opponent's lead Pokémon.\n",
        "def lead_matchup_block(player_team, opponent_lead) -> Dict[str,float]:\n",
        "    feats={}\n",
        "    if not player_team or not opponent_lead: return feats\n",
        "    opp_types = [t for t in opponent_lead.get(\"types\",[]) if t!=\"notype\"]\n",
        "    mults=[]\n",
        "    for p in player_team:\n",
        "        for t in p.get(\"types\") or []:\n",
        "            if t!=\"notype\":\n",
        "                mults.append(type_multiplier(t, opp_types)) # Type multipliers for player's types vs opponent lead\n",
        "    if mults:\n",
        "        feats[\"lead_type_avg\"] = float(np.mean(mults))\n",
        "        feats[\"lead_type_max\"] = float(np.max(mults))\n",
        "        feats[\"lead_type_se\"]  = float(sum(1 for m in mults if m>=2.0)) # Count super-effective hits\n",
        "        feats[\"lead_type_res\"] = float(sum(1 for m in mults if m<=0.5)) # Count resisted hits\n",
        "    fastest = max((p.get(\"base_spe\",0) for p in player_team), default=0)\n",
        "    feats[\"lead_speed_gap_max\"] = float(fastest - (opponent_lead.get(\"base_spe\",0) or 0)) # Speed gap with fastest P1 and P2 lead\n",
        "    return feats\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "BsWYjfooXNMh"
      },
      "outputs": [],
      "source": [
        "# Build a dictionary mapping Pokemon names to their detailed information from the training data.\n",
        "# This allows for quick lookup of Pokemon stats and types.\n",
        "POKEMON_BY_NAME = {}\n",
        "for b in train_data:\n",
        "    for p in b.get(\"p1_team_details\") or []:\n",
        "        nm = str(p.get(\"name\",\"\")).lower()\n",
        "        if nm and nm not in POKEMON_BY_NAME:\n",
        "            POKEMON_BY_NAME[nm] = p\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "3KPiRtkNXOdf"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Approximate the opponent's full team based on Pokemon seen in the battle timeline.\n",
        "# This helps infer the full composition of the opponent's team.\n",
        "def approximate_p2_team(timeline: List[Dict[str, Any]]) -> List[Dict[str,Any]]:\n",
        "    team, seen = [], set()\n",
        "    for nm in species_seen_in_timeline(timeline):\n",
        "        if nm in POKEMON_BY_NAME and nm not in seen: # Only add if details are known and not already added\n",
        "            team.append(POKEMON_BY_NAME[nm]); seen.add(nm)\n",
        "    return team\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "ckF7cdcQXP_O"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Extract features describing overall type advantages and disadvantages between teams.\n",
        "# This assesses the offensive and defensive capabilities of each team based on types.\n",
        "def type_matchup_block(player_team, opponent_team_approx) -> Dict[str,float]:\n",
        "    feats={}\n",
        "    p_off, o_off = [], [] # Player 1 offense, Opponent offense\n",
        "    for atk in (player_team or []):\n",
        "        atk_types = [t for t in atk.get(\"types\",[]) if t!=\"notype\"]\n",
        "        for d in (opponent_team_approx or []):\n",
        "            def_types = [t for t in d.get(\"types\",[]) if t!=\"notype\"]\n",
        "            for t in atk_types:\n",
        "                p_off.append(type_multiplier(t, def_types)) # Player's offensive type multipliers\n",
        "    for atk in (opponent_team_approx or []):\n",
        "        atk_types = [t for t in atk.get(\"types\",[]) if t!=\"notype\"]\n",
        "        for d in (player_team or []):\n",
        "            def_types = [t for t in d.get(\"types\",[]) if t!=\"notype\"]\n",
        "            for t in atk_types:\n",
        "                o_off.append(type_multiplier(t, def_types)) # Opponent's offensive type multipliers\n",
        "    if p_off:\n",
        "        feats[\"p1_type_avg\"] = float(np.mean(p_off))\n",
        "        feats[\"p1_type_max\"] = float(np.max(p_off))\n",
        "        feats[\"p1_type_se\"]  = float(sum(1 for m in p_off if m>=2.0))\n",
        "        feats[\"p1_type_res\"] = float(sum(1 for m in p_off if m<=0.5))\n",
        "    if o_off:\n",
        "        feats[\"p2_type_avg\"] = float(np.mean(o_off))\n",
        "        feats[\"p2_type_se\"]  = float(sum(1 for m in o_off if m>=2.0))\n",
        "    feats[\"type_adv\"] = feats.get(\"p1_type_avg\",1.0) - feats.get(\"p2_type_avg\",1.0) # Overall type advantage\n",
        "    feats[\"type_se_diff\"] = feats.get(\"p1_type_se\",0.0) - feats.get(\"p2_type_se\",0.0) # Super effective hit difference\n",
        "    return feats\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "K1bs57EK5zB3"
      },
      "outputs": [],
      "source": [
        "# Calculate differences in average base stats between the player's team and the approximated opponent's team.\n",
        "# This provides a direct comparison of raw power and bulk.\n",
        "\n",
        "def base_stat_diffs_vs_team(player_team: List[Dict[str, Any]], opponent_team_approx: List[Dict[str, Any]]) -> Dict[str, float]:\n",
        "    feats = {}\n",
        "    keys = [\"hp\",\"atk\",\"def\",\"spa\",\"spd\",\"spe\",\"special\"]\n",
        "    for k in keys:\n",
        "        feats[f\"base_{k}_diff_vs_team\"] = 0.0\n",
        "    feats[\"p2_seen_count\"] = float(len(opponent_team_approx or []))\n",
        "    if not player_team or not opponent_team_approx:\n",
        "        return feats\n",
        "\n",
        "    cols = [\"base_hp\",\"base_atk\",\"base_def\",\"base_spa\",\"base_spd\",\"base_spe\"]\n",
        "\n",
        "    p1_df = pd.DataFrame([{c: float(p.get(c, 0) or 0) for c in cols} for p in player_team])\n",
        "    p2_df = pd.DataFrame([{c: float(p.get(c, 0) or 0) for c in cols} for p in opponent_team_approx])\n",
        "\n",
        "    p1_avg = p1_df.mean(axis=0)\n",
        "    p2_avg = p2_df.mean(axis=0)\n",
        "\n",
        "    # Calculate stat differences\n",
        "    feats[\"base_hp_diff_vs_team\"]  = float(p1_avg[\"base_hp\"]  - p2_avg[\"base_hp\"])\n",
        "    feats[\"base_atk_diff_vs_team\"] = float(p1_avg[\"base_atk\"] - p2_avg[\"base_atk\"])\n",
        "    feats[\"base_def_diff_vs_team\"] = float(p1_avg[\"base_def\"] - p2_avg[\"base_def\"])\n",
        "    feats[\"base_spa_diff_vs_team\"] = float(p1_avg[\"base_spa\"] - p2_avg[\"base_spa\"])\n",
        "    feats[\"base_spd_diff_vs_team\"] = float(p1_avg[\"base_spd\"] - p2_avg[\"base_spd\"])\n",
        "    feats[\"base_spe_diff_vs_team\"] = float(p1_avg[\"base_spe\"] - p2_avg[\"base_spe\"])\n",
        "\n",
        "    p1_special = 0.5 * (p1_avg[\"base_spa\"] + p1_avg[\"base_spd\"])\n",
        "    p2_special = 0.5 * (p2_avg[\"base_spa\"] + p2_avg[\"base_spd\"])\n",
        "    feats[\"special_diff_vs_team\"] = float(p1_special - p2_special)\n",
        "\n",
        "    # Max speed and bulk differences\n",
        "    feats[\"base_spe_max_gap_vs_team\"] = float(p1_df[\"base_spe\"].max() - p2_df[\"base_spe\"].max())\n",
        "    feats[\"base_bulk95_gap_vs_team\"] = float(\n",
        "        np.percentile(p1_df[\"base_hp\"]+p1_df[\"base_def\"]+p1_df[\"base_spd\"], 95)\n",
        "        - np.percentile(p2_df[\"base_hp\"]+p2_df[\"base_def\"]+p2_df[\"base_spd\"], 95)\n",
        "    )\n",
        "    return feats\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "E1xsyy3uXRQq"
      },
      "outputs": [],
      "source": [
        "# Generate binary features indicating the presence of specific top species\n",
        "# in the player's team or among the opponent's seen Pokemon.\n",
        "def bag_of_species_features(player_team, opponent_seen) -> Dict[str,float]:\n",
        "    feats={}\n",
        "    player_names = [str(p.get(\"name\",\"\")).lower() for p in (player_team or [])]\n",
        "    player_set = set(player_names)\n",
        "    for s in TOP_SPECIES:\n",
        "        feats[f\"p1_has_{s}\"] = 1.0 if s in player_set else 0.0 # Does player 1 have this species?\n",
        "        feats[f\"p2_seen_{s}\"] = 1.0 if s in opponent_seen else 0.0 # Has this species been seen on opponent's side?\n",
        "    return feats\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "GetlSNo2XSwP"
      },
      "outputs": [],
      "source": [
        "# Calculate basic statistics (mean, std, min, max) for a given array.\n",
        "# Returns default values if the array is empty.\n",
        "def window_stats(arr):\n",
        "    if len(arr)==0:\n",
        "        return {\"mean\":0.0,\"std\":0.0,\"min\":0.0,\"max\":0.0}\n",
        "    return {\"mean\": float(np.mean(arr)), \"std\": float(np.std(arr)), \"min\": float(np.min(arr)), \"max\": float(np.max(arr))}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "wV7X4pWVXT9b"
      },
      "outputs": [],
      "source": [
        "# Extract features describing HP differences and their trends over different time windows in the battle.\n",
        "# This captures dynamic shifts in battle advantage.\n",
        "def hp_windows_features(timeline):\n",
        "    t = timeline or []\n",
        "    p_hp = np.array([safe_float((tr.get(\"p1_pokemon_state\") or {}).get(\"hp_pct\",1.0)) for tr in t], dtype=float)\n",
        "    o_hp = np.array([safe_float((tr.get(\"p2_pokemon_state\") or {}).get(\"hp_pct\",1.0)) for tr in t], dtype=float)\n",
        "    diff = p_hp - o_hp # HP difference (P1 - P2)\n",
        "    deriv = np.diff(diff) if len(diff) >= 2 else np.array([], dtype=float) # Rate of change of HP difference\n",
        "    def band(a,b):\n",
        "        sw = window_stats(diff[a:b] if len(diff) else np.array([]))\n",
        "        sd = window_stats(deriv[a:b] if len(deriv) else np.array([]))\n",
        "        return {\n",
        "            f\"hp_diff_{a}_{b}_mean\": sw[\"mean\"],\n",
        "            f\"hp_diff_{a}_{b}_std\":  sw[\"std\"],\n",
        "            f\"hp_diff_{a}_{b}_min\":  sw[\"min\"],\n",
        "            f\"hp_diff_{a}_{b}_max\":  sw[\"max\"],\n",
        "            f\"hp_mom_{a}_{b}_mean\":  sd[\"mean\"],\n",
        "            f\"hp_mom_{a}_{b}_std\":   sd[\"std\"],\n",
        "        }\n",
        "    feats = {}\n",
        "    # Extract stats for different time windows\n",
        "    feats.update(band(0, min(3, len(diff))))\n",
        "    feats.update(band(0, min(5, len(diff))))\n",
        "    feats.update(band(5, min(10, len(diff))))\n",
        "    return feats\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "lAP0LMWRXVsA"
      },
      "outputs": [],
      "source": [
        "# Identify the timing (turn number) of the first paralysis, sleep, or freeze\n",
        "# inflicted or received by each player, and calculate their differences.\n",
        "def first_status_timing_features(timeline):\n",
        "    t = timeline or []\n",
        "    INF = 999.0 # Use a large number to represent status not inflicted/received\n",
        "    def find_first(status, side):\n",
        "        for i, tr in enumerate(t, start=1):\n",
        "            st = (tr.get(f\"{side}_pokemon_state\") or {}).get(\"status\")\n",
        "            if st == status: return float(i)\n",
        "        return INF\n",
        "    feats={}\n",
        "    for st in (\"par\",\"slp\",\"frz\"):\n",
        "        o_turn = find_first(st, \"p2\") # Turn opponent inflicts status on player\n",
        "        p_turn = find_first(st, \"p1\") # Turn player inflicts status on opponent\n",
        "        feats[f\"first_{st}_we_inflict\"] = o_turn\n",
        "        feats[f\"first_{st}_we_receive\"] = p_turn\n",
        "        feats[f\"first_{st}_diff\"] = p_turn - o_turn\n",
        "    # More explicit diff features\n",
        "    feats[\"first_par_diff\"] = feats.get(\"first_par_we_receive\",999.0) - feats.get(\"first_par_we_inflict\",999.0)\n",
        "    feats[\"first_slp_diff\"] = feats.get(\"first_slp_we_receive\",999.0) - feats.get(\"first_slp_we_inflict\",999.0)\n",
        "    feats[\"first_frz_diff\"] = feats.get(\"first_frz_we_receive\",999.0) - feats.get(\"first_frz_we_inflict\",999.0)\n",
        "    return feats\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gpAJ4Df9wMD"
      },
      "source": [
        "Transform raw battle data into meaningful features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "f4J2bMiKXYcp"
      },
      "outputs": [],
      "source": [
        "# Extract core timeline features such as battle length, faints, HP differentials,\n",
        "# offensive metrics (base power, effective damage), boosts, and status turns.\n",
        "def timeline_core_features(timeline) -> Dict[str,float]:\n",
        "    feats = {}\n",
        "    t = timeline or []\n",
        "    if not t:\n",
        "        # Default features for empty timelines\n",
        "        feats.update({\n",
        "            \"battle_len\":0.0,\"faint_diff\":0.0,\"hp_diff_final\":0.0,\"hp_auc\":0.0,\n",
        "            \"time_in_lead\":0.0,\"first_faint_adv\":0.0,\"effective_damage_diff\":0.0,\n",
        "            \"base_power_diff\":0.0,\"boost_adv\":0.0,\"status_adv\":0.0,\n",
        "            \"eff_damage_ratio\":1.0,\"bp_ratio\":1.0,\n",
        "            \"p1_switches\":0.0, \"p2_switches\":0.0, \"switch_diff\":0.0,\n",
        "            \"p1_status_turns\":0.0, \"p2_status_turns\":0.0,\n",
        "            \"p1_faints\":0.0, \"p2_faints\":0.0,\n",
        "            \"hp_diff_std\":0.0, \"hp_diff_deriv_abs_mean\":0.0,\n",
        "            \"p1_time_below_50\":0.0, \"p2_time_below_50\":0.0, \"below50_diff\":0.0\n",
        "        })\n",
        "        return feats\n",
        "    feats[\"battle_len\"] = float(len(t))\n",
        "    p_sw = o_sw = 0 # Player switches\n",
        "    prev_p = prev_o = None\n",
        "    auc = 0.0; lead = 0.0 # Area under curve for HP diff, time in lead\n",
        "    p_f = o_f = 0 # Faints\n",
        "    p_st = o_st = 0 # Status turns\n",
        "    p_bp = o_bp = 0.0 # Base power dealt\n",
        "    p_eff = o_eff = 0.0 # Effective damage dealt\n",
        "    p_boost = o_boost = 0.0 # Stat boosts\n",
        "    p_f_first = math.inf; o_f_first = math.inf # First faint turn\n",
        "\n",
        "    for idx, turn in enumerate(t, start=1):\n",
        "        ps = turn.get(\"p1_pokemon_state\") or {}\n",
        "        os = turn.get(\"p2_pokemon_state\") or {}\n",
        "        pm = turn.get(\"p1_move_details\")\n",
        "        om = turn.get(\"p2_move_details\")\n",
        "        pn = ps.get(\"name\"); on = os.get(\"name\")\n",
        "\n",
        "        # Count switches\n",
        "        if prev_p is not None and pn and pn != prev_p: p_sw += 1\n",
        "        if prev_o is not None and on and on != prev_o: o_sw += 1\n",
        "        prev_p, prev_o = pn, on\n",
        "\n",
        "        php = safe_float(ps.get(\"hp_pct\",1.0), 1.0)\n",
        "        ohp = safe_float(os.get(\"hp_pct\",1.0), 1.0)\n",
        "        d = php - ohp # Current HP difference\n",
        "        auc += d # Accumulate for AUC\n",
        "        if d > 0: lead += 1 # Accumulate time in lead\n",
        "\n",
        "        # Count status turns\n",
        "        pstatus = ps.get(\"status\"); ostatus = os.get(\"status\")\n",
        "        if pstatus and pstatus not in (\"nostatus\",\"fnt\"): p_st += 1\n",
        "        if ostatus and ostatus not in (\"nostatus\",\"fnt\"): o_st += 1\n",
        "        if pstatus == \"fnt\":\n",
        "            p_f += 1\n",
        "            if p_f_first == math.inf: p_f_first = idx\n",
        "        if ostatus == \"fnt\":\n",
        "            o_f += 1\n",
        "            if o_f_first == math.inf: o_f_first = idx\n",
        "\n",
        "        # Accumulate stat boosts\n",
        "        p_boost += sum_boosts(ps.get(\"boosts\"))\n",
        "        o_boost += sum_boosts(os.get(\"boosts\"))\n",
        "\n",
        "        # Calculate base power and effective damage\n",
        "        if pm and float(pm.get(\"base_power\") or 0) > 0:\n",
        "            adj = stab_adjusted_bp(pm, str(pn or \"\").lower())\n",
        "            mt = str(pm.get(\"type\") or \"\").lower()\n",
        "            otypes = [x for x in os.get(\"types\",[]) if x!=\"notype\"]\n",
        "            p_bp += adj\n",
        "            p_eff += adj * type_multiplier(mt, otypes)\n",
        "        if om and float(om.get(\"base_power\") or 0) > 0:\n",
        "            adj = stab_adjusted_bp(om, str(on or \"\").lower())\n",
        "            mt = str(om.get(\"type\") or \"\").lower()\n",
        "            ptypes = [x for x in ps.get(\"types\",[]) if x!=\"notype\"]\n",
        "            o_bp += adj\n",
        "            o_eff += adj * type_multiplier(mt, ptypes)\n",
        "\n",
        "    feats[\"p1_switches\"] = float(p_sw)\n",
        "    feats[\"p2_switches\"] = float(o_sw)\n",
        "    feats[\"switch_diff\"] = float(o_sw - p_sw)\n",
        "    feats[\"hp_auc\"] = float(auc)\n",
        "    feats[\"time_in_lead\"] = float(lead)\n",
        "    # Final HP difference at the end of the battle\n",
        "    feats[\"hp_diff_final\"] = float(\n",
        "        safe_float((t[-1].get(\"p1_pokemon_state\") or {}).get(\"hp_pct\",1.0)) -\n",
        "        safe_float((t[-1].get(\"p2_pokemon_state\") or {}).get(\"hp_pct\",1.0))\n",
        "    )\n",
        "    feats[\"p1_status_turns\"] = float(p_st)\n",
        "    feats[\"p2_status_turns\"] = float(o_st)\n",
        "    feats[\"status_adv\"] = float(o_st - p_st)\n",
        "    feats[\"p1_faints\"] = float(p_f)\n",
        "    feats[\"p2_faints\"] = float(o_f)\n",
        "    feats[\"faint_diff\"] = float(o_f - p_f)\n",
        "    feats[\"base_power_diff\"] = float(p_bp - o_bp)\n",
        "    feats[\"effective_damage_diff\"] = float(p_eff - o_eff)\n",
        "    feats[\"boost_adv\"] = float(p_boost - o_boost)\n",
        "\n",
        "    pft = p_f_first if p_f_first < math.inf else 999.0\n",
        "    oft = o_f_first if o_f_first < math.inf else 999.0\n",
        "    feats[\"first_faint_adv\"] = float(oft - pft)\n",
        "\n",
        "    p_series = np.array([safe_float((tr.get(\"p1_pokemon_state\") or {}).get(\"hp_pct\",1.0)) for tr in t])\n",
        "    o_series = np.array([safe_float((tr.get(\"p2_pokemon_state\") or {}).get(\"hp_pct\",1.0)) for tr in t])\n",
        "    diff_series = p_series - o_series\n",
        "    feats[\"hp_diff_std\"] = float(np.std(diff_series)) if len(diff_series)>1 else 0.0\n",
        "    feats[\"hp_diff_deriv_abs_mean\"] = float(np.mean(np.abs(np.diff(diff_series)))) if len(diff_series)>2 else 0.0\n",
        "\n",
        "    # Calculate time spent below certain HP thresholds\n",
        "    def time_below(arr, thr):\n",
        "      if len(arr)==0: return 0.0\n",
        "      return float(np.mean(arr <= thr))\n",
        "\n",
        "    feats[\"p1_time_below_50\"] = time_below(p_series, 0.5)\n",
        "    feats[\"p2_time_below_50\"] = time_below(o_series, 0.5)\n",
        "    feats[\"below50_diff\"] = feats[\"p2_time_below_50\"] - feats[\"p1_time_below_50\"]\n",
        "    feats[\"eff_damage_ratio\"] = (p_eff + 1e-3) / (o_eff + 1e-3)\n",
        "    feats[\"bp_ratio\"] = (p_bp + 1e-3) / (o_bp + 1e-3)\n",
        "    return feats\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "Gy3eZ4lJXclH"
      },
      "outputs": [],
      "source": [
        "# Create interaction features by combining existing features to capture more complex battle dynamics.\n",
        "def interaction_features(fd: Dict[str,float]) -> Dict[str,float]:\n",
        "    out={}\n",
        "    # Example interaction: Faint difference combined with final HP difference\n",
        "    if \"faint_diff\" in fd and \"hp_diff_final\" in fd:\n",
        "        out[\"faint_hp_compound\"] = fd[\"faint_diff\"] * fd[\"hp_diff_final\"]\n",
        "    # Status advantage combined with effective damage difference\n",
        "    if \"status_adv\" in fd and \"effective_damage_diff\" in fd:\n",
        "        out[\"status_damage_compound\"] = fd[\"status_adv\"] * fd[\"effective_damage_diff\"]\n",
        "    # Type advantage combined with effective damage difference\n",
        "    if \"type_adv\" in fd and \"effective_damage_diff\" in fd:\n",
        "        out[\"type_damage_synergy\"] = fd[\"type_adv\"] * fd[\"effective_damage_diff\"]\n",
        "    # HP area under curve combined with time in lead for control score\n",
        "    if \"hp_auc\" in fd and \"time_in_lead\" in fd:\n",
        "        out[\"lead_control_score\"] = fd[\"hp_auc\"] * (1.0 + 0.01 * fd[\"time_in_lead\"])\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "-p9f9IiVXhWZ"
      },
      "outputs": [],
      "source": [
        "# Extract features related to player's Pokémon speed relative to opponent's lead,\n",
        "# indicating potential for"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "CgD3Ay_NXjAx"
      },
      "outputs": [],
      "source": [
        "# Add compact polynomial features and apply clipping to some features to handle outliers.\n",
        "def add_compact_polys(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    new_df = df.copy()\n",
        "    # Calculate faint ratio, handling division by zero\n",
        "    if {\"p1_faints\",\"p2_faints\"}.issubset(new_df.columns):\n",
        "        new_df[\"faint_ratio_safe\"] = (new_df[\"p2_faints\"] + 0.5) / (new_df[\"p1_faints\"] + 0.5)\n",
        "    # Calculate lead share (proportion of battle in lead), handling division by zero\n",
        "    if {\"time_in_lead\",\"battle_len\"}.issubset(new_df.columns):\n",
        "        new_df[\"lead_share\"] = np.where(new_df[\"battle_len\"] > 0, new_df[\"time_in_lead\"] / new_df[\"battle_len\"], 0.0)\n",
        "    # Normalize HP Area Under Curve\n",
        "    if {\"hp_auc\",\"battle_len\"}.issubset(new_df.columns):\n",
        "        new_df[\"hp_auc_norm\"] = np.where(new_df[\"battle_len\"] > 0, new_df[\"hp_auc\"]/new_df[\"battle_len\"], 0.0)\n",
        "    # Clip certain columns to the 1st and 99th percentiles to reduce outlier impact\n",
        "    for col in (\"hp_diff_std\",\"hp_diff_deriv_abs_mean\",\"effective_damage_diff\",\"hp_auc\",\"lead_control_score\"):\n",
        "        if col in new_df.columns:\n",
        "            new_df[col] = np.clip(new_df[col], np.percentile(new_df[col],1), np.percentile(new_df[col],99))\n",
        "    # Define columns for polynomial feature generation\n",
        "    poly_cols = [c for c in [\n",
        "        \"lead_share\",\"hp_auc_norm\",\"effective_damage_diff\",\"base_power_diff\",\n",
        "        \"type_adv\",\"lead_control_score\",\"faint_diff\",\"status_adv\",\"lead_speed_gap_max\",\n",
        "        \"eff_damage_ratio\",\"bp_ratio\",\"below50_diff\"\n",
        "    ] if c in new_df.columns]\n",
        "    # Generate polynomial features if relevant columns exist\n",
        "    if poly_cols:\n",
        "        poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "        feats = poly.fit_transform(new_df[poly_cols])\n",
        "        names = [f\"poly_{n}\" for n in poly.get_feature_names_out(poly_cols)]\n",
        "        new_df = pd.concat([new_df, pd.DataFrame(feats, columns=names, index=new_df.index)], axis=1)\n",
        "    return new_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRjxyPdf99uQ"
      },
      "source": [
        "Captures who's winning right now"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "cNS1ZmqWXkts"
      },
      "outputs": [],
      "source": [
        "# Calculate momentum features based on HP differences over various turn windows.\n",
        "def momentum_features(timeline):\n",
        "    feats={}\n",
        "    timeline = timeline or []\n",
        "    if len(timeline) < 1:\n",
        "        # Initialize momentum features to 0 if timeline is too short\n",
        "        for cp in [5,10,15,20,25]:\n",
        "            feats[f\"momentum_{cp}\"]=0.0\n",
        "            feats[f\"momentum_trend_{cp}\"]=0.0\n",
        "        return feats\n",
        "    for cp in [5,10,15,20,25]: # Iterate through different capture points (turn cutoffs)\n",
        "        if cp <= len(timeline):\n",
        "            turns = timeline[:cp] # Consider turns up to the current capture point\n",
        "            diffs=[]\n",
        "            for tr in turns:\n",
        "                p1 = safe_float((tr.get(\"p1_pokemon_state\") or {}).get(\"hp_pct\",1.0))\n",
        "                p2 = safe_float((tr.get(\"p2_pokemon_state\") or {}).get(\"hp_pct\",1.0))\n",
        "                diffs.append(p1 - p2) # Calculate HP difference (P1 - P2)\n",
        "            # Calculate mean HP difference for the last few turns (momentum)\n",
        "            feats[f\"momentum_{cp}\"] = float(np.mean(diffs[-3:])) if len(diffs)>=1 else 0.0\n",
        "            # Calculate trend of HP difference for the last few turns (momentum trend)\n",
        "            feats[f\"momentum_trend_{cp}\"] = float(np.mean(np.diff(diffs[-4:]))) if len(diffs) > 3 else 0.0\n",
        "        else:\n",
        "            # Set momentum features to 0 if the timeline doesn't reach the capture point\n",
        "            feats[f\"momentum_{cp}\"]=0.0\n",
        "            feats[f\"momentum_trend_{cp}\"]=0.0\n",
        "    return feats\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "XbRczPYJXlyB"
      },
      "outputs": [],
      "source": [
        "# Extract features related to comebacks and largest HP swings in a battle.\n",
        "def critical_turn_features(timeline):\n",
        "    feats={\"p1_comebacks\":0.0,\"p2_comebacks\":0.0,\"largest_swing_p1\":0.0,\"largest_swing_p2\":0.0}\n",
        "    timeline = timeline or []\n",
        "    if len(timeline) < 2: return feats # Need at least two turns to calculate changes\n",
        "    diffs=[]\n",
        "    for tr in timeline:\n",
        "        p1 = safe_float((tr.get(\"p1_pokemon_state\") or {}).get(\"hp_pct\",1.0))\n",
        "        p2 = safe_float((tr.get(\"p2_pokemon_state\") or {}).get(\"hp_pct\",1.0))\n",
        "        diffs.append(p1 - p2) # Store HP differences for each turn\n",
        "    changes = np.diff(diffs) # Calculate turn-over-turn changes in HP difference\n",
        "    feats[\"largest_swing_p1\"] = float(np.max(changes)) if len(changes)>0 else 0.0 # Largest positive swing for P1\n",
        "    feats[\"largest_swing_p2\"] = float(np.min(changes)) if len(changes)>0 else 0.0 # Largest negative swing for P1 (positive for P2)\n",
        "    # Count comebacks where the lead changes from negative to positive for P1 or vice versa\n",
        "    for i in range(1, len(diffs)):\n",
        "        if diffs[i-1] < 0 and diffs[i] > 0: feats[\"p1_comebacks\"] += 1\n",
        "        elif diffs[i-1] > 0 and diffs[i] < 0: feats[\"p2_comebacks\"] += 1\n",
        "    return feats\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "HS5x8uBEXnER"
      },
      "outputs": [],
      "source": [
        "# Calculate features related to super-effective and resisted hits for both players.\n",
        "def move_effectiveness_features(timeline):\n",
        "    feats={}\n",
        "    p1_se = p2_se = 0 # Player 1 and Player 2 super-effective hits\n",
        "    p1_res = p2_res = 0 # Player 1 and Player 2 resisted hits\n",
        "    for tr in timeline or []:\n",
        "        p1m = tr.get(\"p1_move_details\"); p2m = tr.get(\"p2_move_details\")\n",
        "        p1s = tr.get(\"p1_pokemon_state\") or {}\n",
        "        p2s = tr.get(\"p2_pokemon_state\") or {}\n",
        "        # Check P1's move effectiveness\n",
        "        if p1m and float(p1m.get(\"base_power\",0)) > 0:\n",
        "            mtype = str(p1m.get(\"type\",\"\")).lower()\n",
        "            def_types = [t for t in p2s.get(\"types\", []) if t != \"notype\"]\n",
        "            mult = type_multiplier(mtype, def_types)\n",
        "            if mult >= 2.0: p1_se += 1 # Super-effective hit\n",
        "            elif mult <= 0.5: p1_res += 1 # Resisted hit\n",
        "        # Check P2's move effectiveness\n",
        "        if p2m and float(p2m.get(\"base_power\",0)) > 0:\n",
        "            mtype = str(p2m.get(\"type\",\"\")).lower()\n",
        "            def_types = [t for t in p1s.get(\"types\", []) if t != \"notype\"]\n",
        "            mult = type_multiplier(mtype, def_types)\n",
        "            if mult >= 2.0: p2_se += 1\n",
        "            elif mult <= 0.5: p2_res += 1\n",
        "    feats[\"effective_hits_diff\"] = float(p1_se - p2_se) # Difference in super-effective hits\n",
        "    feats[\"resisted_hits_diff\"] = float(p2_res - p1_res) # Difference in resisted hits\n",
        "    feats[\"hit_quality_score\"] = feats[\"effective_hits_diff\"] - feats[\"resisted_hits_diff\"] # Combined quality score\n",
        "    return feats\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "bCElmjmG3ZeN"
      },
      "outputs": [],
      "source": [
        "def crit_pressure_features(player_team, opponent_lead):\n",
        "    sp = [p.get(\"base_spe\",0) for p in (player_team or [])]\n",
        "    p80 = np.percentile(sp, 80) if sp else 0.0\n",
        "    gap = (max(sp) if sp else 0.0) - float((opponent_lead or {}).get(\"base_spe\",0))\n",
        "    return {\"crit_pressure_p80\": float(p80), \"lead_crit_gap\": float(gap)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "GyvT-z_7XoiT"
      },
      "outputs": [],
      "source": [
        "# Orchestrates the extraction of a comprehensive set of features from a single battle record.\n",
        "def extract_example_features(battle: Dict[str,Any], max_turn=None) -> Dict[str,float]:\n",
        "    feats = {\"battle_id\": battle.get(\"battle_id\")}\n",
        "    pteam = battle.get(\"p1_team_details\") or []\n",
        "    opp_lead = battle.get(\"p2_lead_details\") or {}\n",
        "    timeline = battle.get(\"battle_timeline\") or []\n",
        "    if max_turn is not None:\n",
        "        timeline = [t for t in timeline if int(t.get(\"turn\",0)) <= max_turn] # Truncate timeline if max_turn is specified\n",
        "    opp_approx = approximate_p2_team(timeline) # Approximate opponent's team from timeline\n",
        "\n",
        "    # Add various feature blocks\n",
        "    feats.update(prior_features_for_lead((opp_lead or {}).get(\"name\",\"\")))\n",
        "    feats.update(encode_types_block(pteam, opp_lead))\n",
        "    feats.update(stat_summary_block(pteam, opp_lead))\n",
        "    feats.update(lead_matchup_block(pteam, opp_lead))\n",
        "    feats.update(base_stat_diffs_vs_team(pteam, opp_approx))\n",
        "    feats.update(type_matchup_block(pteam, opp_approx))\n",
        "    feats.update(timeline_core_features(timeline))\n",
        "    feats.update(interaction_features(feats))\n",
        "    feats.update(bag_of_species_features(pteam, species_seen_in_timeline(timeline)))\n",
        "    feats.update(timeline_move_counts(timeline))\n",
        "    feats.update(move_buckets_features(timeline))\n",
        "    feats.update(status_turn_counts(timeline))\n",
        "    feats.update(early_cc_features(timeline, window_size=5))\n",
        "    feats.update(hyper_beam_ko_flags(timeline))\n",
        "    feats.update(crit_pressure_features(pteam, opp_lead)) # This function is not defined in the provided notebook. Assuming it exists elsewhere.\n",
        "    feats.update(hp_windows_features(timeline))\n",
        "    feats.update(first_status_timing_features(timeline))\n",
        "    feats.update(momentum_features(timeline))\n",
        "    feats.update(critical_turn_features(timeline))\n",
        "    feats.update(move_effectiveness_features(timeline))\n",
        "\n",
        "    feats.update(timeline_additions(timeline))\n",
        "    return feats\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "jh6Dy3mxXp1h"
      },
      "outputs": [],
      "source": [
        "# Perform parallel feature extraction for a list of battles, handling train/test data and polynomial feature generation.\n",
        "def parallel_feature_extraction(battles, is_train=True, n_jobs=-1, max_turn=None):\n",
        "    if n_jobs == -1:\n",
        "        n_jobs = multiprocessing.cpu_count() # Use all available CPU cores by default\n",
        "    # Extract features for each battle in parallel using joblib\n",
        "    rows = Parallel(n_jobs=n_jobs, backend='loky')(\n",
        "        delayed(extract_example_features)(b, max_turn=max_turn) for b in tqdm(battles, desc=f\"Feature extraction (max_turn={max_turn})\")\n",
        "    )\n",
        "    y = []\n",
        "    if is_train:\n",
        "        y = [1 if b.get(\"player_won\") else 0 for b in battles] # Extract target variable for training data\n",
        "    df = pd.DataFrame(rows)\n",
        "    ids = df[\"battle_id\"].values\n",
        "    X = df.drop(columns=[\"battle_id\"]).fillna(0.0) # Drop battle_id and fill NaN values with 0.0\n",
        "    X = add_compact_polys(X) # Add polynomial features\n",
        "    return (X, np.array(y), ids) if is_train else (X, ids)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "Z8AzMH9FZXRZ"
      },
      "outputs": [],
      "source": [
        "# Apply Out-of-Fold (OOF) target encoding for presence-based features.\n",
        "# This helps prevent data leakage by computing target statistics only on out-of-fold data.\n",
        "def target_encode_presence_oof(X: pd.DataFrame, y: np.ndarray, prefix=\"p1_has_\", folds=5):\n",
        "    skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=RANDOM_STATE)\n",
        "    cols = [c for c in X.columns if c.startswith(prefix)] # Identify columns to be target encoded\n",
        "    if not cols: return X\n",
        "    te = pd.DataFrame(0.5, index=X.index, columns=[c+\"_te\" for c in cols]) # Initialize target encoding DataFrame\n",
        "    for tr_idx, va_idx in skf.split(X, y):\n",
        "        Xtr = X.iloc[tr_idx][cols]; ytr = y[tr_idx]\n",
        "        means = {}\n",
        "        for c in cols:\n",
        "            # Calculate mean of target for rows where the feature is present (greater than 0)\n",
        "            pos = (((Xtr[c] > 0).astype(int)) & (ytr == 1)).sum()\n",
        "            cnt = (Xtr[c] > 0).sum()\n",
        "            means[c] = (pos + 1.0) / (cnt + 2.0) if cnt > 0 else 0.5 # Apply Laplace smoothing\n",
        "        for c in cols:\n",
        "            # Apply the calculated mean to the validation set, or 0.5 if feature is absent\n",
        "            te.loc[te.index[va_idx], c+\"_te\"] = np.where(X.iloc[va_idx][c] > 0, means[c], 0.5)\n",
        "    return pd.concat([X, te], axis=1) # Concatenate original features with target-encoded features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "ZFV4RNq7ZY32"
      },
      "outputs": [],
      "source": [
        "# Apply target encoding to test data using statistics learned from the training data.\n",
        "def target_encode_presence_test(Xtr: pd.DataFrame, ytr: np.ndarray, Xte: pd.DataFrame, prefix=\"p1_has_\"):\n",
        "    cols = [c for c in Xtr.columns if c.startswith(prefix)] # Identify columns used for target encoding in training data\n",
        "    for c in cols:\n",
        "        if c not in Xte.columns:\n",
        "            Xte[c] = 0.0 # Add missing columns to test data, initializing to 0.0\n",
        "    means = {}\n",
        "    for c in cols:\n",
        "        # Calculate mean of target for rows where the feature is present in training data\n",
        "        pos = (((Xtr[c] > 0).astype(int)) & (ytr == 1)).sum()\n",
        "        cnt = (Xtr[c] > 0).sum()\n",
        "        means[c] = (pos + 1.0) / (cnt + 2.0) if cnt > 0 else 0.5 # Apply Laplace smoothing\n",
        "    for c in cols:\n",
        "        # Apply the calculated mean to the test data, or 0.5 if feature is absent\n",
        "        Xte[c+\"_te\"] = np.where(Xte[c] > 0, means[c], 0.5)\n",
        "    return Xte\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "0EiRPacGZaOn"
      },
      "outputs": [],
      "source": [
        "# Prune features with low variance or high correlation to reduce dimensionality and multicollinearity.\n",
        "def prune_low_var_high_corr(features: pd.DataFrame, correlation_cutoff=0.995, variance_cutoff=1e-10):\n",
        "    # Filter out features with variance below the cutoff\n",
        "    filtered = features.loc[:, features.var(numeric_only=True) > variance_cutoff]\n",
        "    # Calculate absolute correlation matrix\n",
        "    corr = filtered.corr(numeric_only=True).abs()\n",
        "    # Select upper triangle of correlation matrix\n",
        "    up = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
        "    # Identify columns to drop based on high correlation\n",
        "    drop = [c for c in up.columns if any(up[c] > correlation_cutoff)]\n",
        "    return filtered.drop(columns=drop, errors=\"ignore\") # Drop identified columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "o5XOpxyDZhmL"
      },
      "outputs": [],
      "source": [
        "# Get probability predictions from a model, handling different types of model outputs.\n",
        "def model_proba(model, X):\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        return model.predict_proba(X)[:,1] # Return probability of the positive class\n",
        "    if hasattr(model, \"decision_function\"):\n",
        "        d = model.decision_function(X) # Convert decision function output to probabilities\n",
        "        return 1.0 / (1.0 + np.exp(-d))\n",
        "    pred = model.predict(X).astype(float) # Fallback for models without predict_proba/decision_function\n",
        "    return pred * 0.999 + 0.0005 # Scale to approximate probabilities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "V_1i6vYyZikT"
      },
      "outputs": [],
      "source": [
        "# Fit a list of models to the provided data.\n",
        "def fit_models(model_list, X, y):\n",
        "    fitted=[]\n",
        "    for m in tqdm(model_list, desc=\"Training base models\"):\n",
        "        m.fit(X, y) # Train each model\n",
        "        fitted.append(m)\n",
        "    return fitted\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "2Znh2k6wZjWD"
      },
      "outputs": [],
      "source": [
        "\n",
        "def stack_probas(model_list, X) -> np.ndarray:\n",
        "    # Stack probability predictions from multiple models into a single NumPy array.\n",
        "    return np.vstack([model_proba(m, X) for m in model_list])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zc-2-Nxh-nNr"
      },
      "source": [
        "For each model select most predictive features to prevent overfitting and generate unbiased predictions on training data using cross-validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "Y9ueu7BkZo9G"
      },
      "outputs": [],
      "source": [
        "# Perform Out-of-Fold (OOF) stacking with inner feature selection for base models.\n",
        "# This method ensures that feature selection is done independently for each fold,\n",
        "# preventing data leakage and producing unbiased OOF predictions.\n",
        "def oof_stack_with_inner_fs(model_builder, X, y, k_features=450, folds=5):\n",
        "    skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=RANDOM_STATE)\n",
        "    num_models = len(model_builder()) # Get the number of base models\n",
        "    N = len(y)\n",
        "    oof = np.zeros((num_models, N), dtype=float) # Array to store OOF predictions\n",
        "    selected_cols_union = set() # To store the union of selected columns across all folds\n",
        "\n",
        "    for _, (tr, va) in enumerate(tqdm(skf.split(X, y), total=folds, desc=\"OOF Stacking + inner FS\")):\n",
        "        vt = VarianceThreshold(1e-6) # Remove features with very low variance\n",
        "        Xtr_vt = vt.fit_transform(X.iloc[tr])\n",
        "        cols_vt = X.columns[vt.get_support()]\n",
        "\n",
        "        skb = SelectKBest(mutual_info_classif, k=min(k_features, len(cols_vt))) # Select top K features based on mutual information\n",
        "        skb.fit(Xtr_vt, y[tr])\n",
        "        cols_fold = cols_vt[skb.get_support()] # Features selected for the current fold\n",
        "        selected_cols_union.update(cols_fold) # Add to the union of selected columns\n",
        "\n",
        "        Xtr = X.iloc[tr][cols_fold] # Training data for the current fold with selected features\n",
        "        Xva = X.iloc[va][cols_fold] # Validation data for the current fold with selected features\n",
        "\n",
        "        fold_models = model_builder() # Re-initialize models for each fold to ensure independence\n",
        "        for m in fold_models:\n",
        "            m.fit(Xtr, y[tr]) # Train base models on the training fold\n",
        "        fold_probs = [model_proba(m, Xva) for m in fold_models] # Get predictions on the validation fold\n",
        "        for mi, pr in enumerate(fold_probs):\n",
        "            oof[mi, va] = pr # Store OOF predictions\n",
        "\n",
        "    selected_cols_union = list(sorted(selected_cols_union)) # Convert to sorted list\n",
        "    return oof, selected_cols_union\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0UPYQW4-6Kt"
      },
      "source": [
        "Find optimal probability threshold for converting predictions to binary decisions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "kzDYXdcvZsqr"
      },
      "outputs": [],
      "source": [
        "# Find the optimal probability threshold for converting continuous predictions into binary classifications.\n",
        "def tune_threshold(probs: np.ndarray, y: np.ndarray, low=0.2, high=0.8, steps=601) -> float:\n",
        "    grid = np.linspace(low, high, steps) # Create a grid of possible thresholds\n",
        "    best_t, best_acc = 0.5, -1.0 # Initialize best threshold and accuracy\n",
        "    for t in grid:\n",
        "        acc = accuracy_score(y, (probs >= t).astype(int)) # Calculate accuracy for current threshold\n",
        "        if acc > best_acc:\n",
        "            best_acc, best_t = acc, float(t) # Update if a better threshold is found\n",
        "    return best_t\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_fEAIGv-Wbx"
      },
      "source": [
        "Split features into 2 groups to create model diversity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "obRYGDnE7q32"
      },
      "outputs": [],
      "source": [
        "# Builds feature views (subsets of features) to create diversity among base models.\n",
        "def build_feature_views(df: pd.DataFrame) -> Dict[str, List[str]]:\n",
        "    cols = list(df.columns)\n",
        "\n",
        "    views = {\n",
        "        \"timeline\": [],\n",
        "        \"combat\": [],\n",
        "    }\n",
        "\n",
        "    # Assign features to 'timeline' or 'combat' view based on their prefixes\n",
        "    for c in cols:\n",
        "        if c.startswith((\n",
        "            \"battle_len\", \"hp_\", \"hp_diff_\", \"hp_mom_\",\n",
        "            \"time_in_lead\", \"lead_share\", \"hp_auc\", \"hp_auc_norm\",\n",
        "            \"lead_control_score\", \"switch_\", \"p1_time_below_50\",\n",
        "            \"p2_time_below_50\", \"below50_diff\",\n",
        "            \"faint_diff\", \"p1_faints\", \"p2_faints\",\n",
        "            \"status_adv\", \"p1_status_turns\", \"p2_status_turns\",\n",
        "            \"first_faint_adv\", \"first_par_\", \"first_slp_\", \"first_frz_\",\n",
        "            \"early_status_score\", \"early_sleep_hits\", \"early_freeze_hits\",\n",
        "            \"p1_hb_ko\", \"p2_hb_ko\",\n",
        "            \"p1_life_turn30\", \"p2_life_turn30\", \"life_diff_turn30\",\n",
        "            \"par_diff\", \"slp_diff\", \"frz_diff\",\n",
        "            \"first_cc_winner\"\n",
        "        )):\n",
        "            views[\"timeline\"].append(c)\n",
        "        else:\n",
        "            views[\"combat\"].append(c)\n",
        "\n",
        "    # Remove any empty views\n",
        "    views = {name: v for name, v in views.items() if len(v) > 0}\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"FEATURE VIEWS:\")\n",
        "    print(\"=\"*60)\n",
        "    for name, v in views.items():\n",
        "        print(f\"  {name:15}: {len(v):4} features\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "    return views\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "6wirzdJ2ZxZm"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Execute the feature extraction, preprocessing, and base model training for a given turn cutoff.\n",
        "def run_one_cutoff(max_turn, builder):\n",
        "    # Extract features for training and testing data\n",
        "    Xtr, ytr, _ = parallel_feature_extraction(\n",
        "        train_data, is_train=True, n_jobs=-1, max_turn=max_turn\n",
        "    )\n",
        "    Xte, ids = parallel_feature_extraction(\n",
        "        test_data, is_train=False, n_jobs=-1, max_turn=max_turn\n",
        "    )\n",
        "\n",
        "    # Apply target encoding to training data (Out-of-Fold to prevent leakage)\n",
        "    Xtr = target_encode_presence_oof(Xtr, ytr, prefix=\"p1_has_\")\n",
        "    Xtr = target_encode_presence_oof(Xtr, ytr, prefix=\"p2_seen_\")\n",
        "\n",
        "    # Reindex test data to match training columns and apply target encoding\n",
        "    Xte = Xte.reindex(columns=Xtr.columns, fill_value=0.0)\n",
        "    Xte = target_encode_presence_test(Xtr, ytr, Xte, prefix=\"p1_has_\")\n",
        "    Xte = target_encode_presence_test(Xtr, ytr, Xte, prefix=\"p2_seen_\")\n",
        "\n",
        "    # Prune features in training data based on variance and correlation\n",
        "    Xtr = prune_low_var_high_corr(Xtr)\n",
        "    # Reindex test data again to match the pruned training columns\n",
        "    Xte = Xte.reindex(columns=Xtr.columns, fill_value=0.0)\n",
        "\n",
        "    views = build_feature_views(Xtr) # Define feature views for diverse base models\n",
        "\n",
        "    all_oof_blocks = [] # Stores OOF predictions from base models\n",
        "    all_train_blocks = [] # Stores predictions on full training set from base models\n",
        "    all_test_blocks = [] # Stores predictions on test set from base models\n",
        "\n",
        "    for view_name, view_cols in views.items():\n",
        "        print(f\"\\n=== View '{view_name}' with {len(view_cols)} features ===\")\n",
        "\n",
        "        Xtr_view = Xtr[view_cols]\n",
        "        Xte_view = Xte[view_cols]\n",
        "\n",
        "        # Perform OOF stacking with inner feature selection for the current view\n",
        "        base_oof_view, selected_cols_union = oof_stack_with_inner_fs(\n",
        "            builder,\n",
        "            Xtr_view,\n",
        "            ytr,\n",
        "            k_features=min(800, len(view_cols)),\n",
        "            folds=5\n",
        "        )\n",
        "\n",
        "        selected_cols_union = list(selected_cols_union)\n",
        "        Xtr_sel_view = Xtr_view[selected_cols_union]\n",
        "        Xte_sel_view = Xte_view.reindex(columns=selected_cols_union, fill_value=0.0)\n",
        "\n",
        "        print(f\"View '{view_name}': {len(selected_cols_union)} columns after inner FS\")\n",
        "\n",
        "        # Train final base models for the current view on the full training set (with selected features)\n",
        "        final_base_models_view = fit_models(builder(), Xtr_sel_view, ytr)\n",
        "\n",
        "        # Get predictions from the final base models\n",
        "        train_base_probs_view = stack_probas(final_base_models_view, Xtr_sel_view)\n",
        "        test_base_probs_view  = stack_probas(final_base_models_view, Xte_sel_view)\n",
        "\n",
        "        all_oof_blocks.append(base_oof_view)\n",
        "        all_train_blocks.append(train_base_probs_view)\n",
        "        all_test_blocks.append(test_base_probs_view)\n",
        "\n",
        "    # Concatenate predictions from all views\n",
        "    base_oof_probs = np.vstack(all_oof_blocks)\n",
        "    train_base_probs = np.vstack(all_train_blocks)\n",
        "    test_base_probs  = np.vstack(all_test_blocks)\n",
        "\n",
        "    print(f\"\\n[cutoff={max_turn}] Total base models after views: {base_oof_probs.shape[0]}\")\n",
        "\n",
        "    return {\n",
        "        \"Xtr_sel\": Xtr,\n",
        "        \"Xte_sel\": Xte,\n",
        "        \"ytr\": ytr,\n",
        "        \"test_ids\": ids,\n",
        "        \"base_oof_probs\": base_oof_probs,\n",
        "        \"train_base_probs\": train_base_probs,\n",
        "        \"test_base_probs\": test_base_probs,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlK-BRZpZ7LH",
        "outputId": "7dbeef27-bb7f-4cf0-ab58-983052ac0289"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRACK_MOVES size: 33\n"
          ]
        }
      ],
      "source": [
        "# Recalculate and print the size of the tracked moves vocabulary.\n",
        "TRACK_MOVES, MOVE_FREQ = build_move_vocab_by_coverage(train_data, target_coverage=0.985, min_freq=3)\n",
        "print(f\"TRACK_MOVES size: {len(TRACK_MOVES)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEv7pIxpHbER"
      },
      "source": [
        "-----"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "jr06rjo8IZZd"
      },
      "outputs": [],
      "source": [
        "# Define a function to construct a list of diverse base models for stacking.\n",
        "def build_base_models():\n",
        "\n",
        "    models = [\n",
        "        # Ridge Classifier with StandardScaler for regularization\n",
        "        Pipeline([(\"scaler\", StandardScaler()), (\"ridge\", RidgeClassifierCV(alphas=np.logspace(-3, 3, 20)))]),\n",
        "\n",
        "        # Quadratic Discriminant Analysis with a regularization parameter\n",
        "        Pipeline([(\"scaler\", StandardScaler()), (\"qda\", QuadraticDiscriminantAnalysis(reg_param=0.01))]),\n",
        "\n",
        "        # Logistic Regression with L2 regularization and increased max_iter\n",
        "        Pipeline([(\"scaler\", StandardScaler()), (\"logreg\", LogisticRegression(C=1.0, max_iter=2000, random_state=RANDOM_STATE))]),\n",
        "\n",
        "        # Gradient Boosting Classifier (tree-based ensemble)\n",
        "        GradientBoostingClassifier(n_estimators=300, learning_rate=0.05, max_depth=5, random_state=RANDOM_STATE),\n",
        "\n",
        "        # Support Vector Classifier with RBF kernel and probability calibration\n",
        "        Pipeline([(\"scaler\", StandardScaler()), (\"svc\", SVC(kernel=\"rbf\", C=2.5, gamma=\"scale\", probability=True, random_state=RANDOM_STATE))]),\n",
        "\n",
        "        # Random Forest Classifier with specified estimators, depth, and min_samples_split\n",
        "        RandomForestClassifier(n_estimators=500, max_depth=15, min_samples_split=10, n_jobs=-1, random_state=RANDOM_STATE),\n",
        "        # AdaBoost Classifier (boosting ensemble)\n",
        "        AdaBoostClassifier(n_estimators=200, learning_rate=0.8, random_state=RANDOM_STATE),\n",
        "\n",
        "        # Calibrated Extra Trees Classifier for robust probability predictions\n",
        "        CalibratedClassifierCV(\n",
        "            estimator=ExtraTreesClassifier(\n",
        "                n_estimators=1200, max_depth=None, max_features=0.6,\n",
        "                min_samples_split=4, min_samples_leaf=2, bootstrap=True,\n",
        "                n_jobs=-1, random_state=RANDOM_STATE\n",
        "            ),\n",
        "            method=\"isotonic\", cv=3\n",
        "        ),\n",
        "\n",
        "        # Histogram-based Gradient Boosting Classifier for speed and performance\n",
        "        HistGradientBoostingClassifier(max_iter=500, learning_rate=0.03, max_depth=10, l2_regularization=1.0, random_state=RANDOM_STATE),\n",
        "\n",
        "        # Linear Discriminant Analysis\n",
        "        Pipeline([(\"scaler\", StandardScaler()), (\"lda\", LinearDiscriminantAnalysis())]), # Adding LDA based on output from m3Nv2gTivPMc\n",
        "\n",
        "    ]\n",
        "\n",
        "    return tuple(models)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "xTJuhKLaIgY_"
      },
      "outputs": [],
      "source": [
        "# Assign the base model builder function to a variable 'builder' for later use.\n",
        "builder = build_base_models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQRPeKSyIgZC"
      },
      "outputs": [],
      "source": [
        "# Iterate through defined turn cutoffs, running the feature extraction and base model training pipeline for each.\n",
        "per_cut = []\n",
        "for N in TURN_CUTOFFS:\n",
        "  per_cut.append(run_one_cutoff(N, builder))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKuBz6voIgZY"
      },
      "source": [
        "-----"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "GjzdR73_vPMV"
      },
      "outputs": [],
      "source": [
        "# Aggregate Out-of-Fold (OOF), training, and test predictions from all cutoffs and base models.\n",
        "base_oof_all = np.vstack([d[\"base_oof_probs\"] for d in per_cut])\n",
        "base_train_all = np.vstack([d[\"train_base_probs\"] for d in per_cut])\n",
        "base_test_all = np.vstack([d[\"test_base_probs\"] for d in per_cut])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "I6cb8JoIvPMY"
      },
      "outputs": [],
      "source": [
        "# Extract the true labels for training and battle IDs for testing from the first cutoff's results.\n",
        "y_train = per_cut[0][\"ytr\"]\n",
        "test_ids = per_cut[0][\"test_ids\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hp9lB7nLvPMZ",
        "outputId": "f0412813-aa95-4817-d0f0-64521bf5af03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Turn cutoffs: [None]\n",
            "Base model types: 10\n",
            "Feature views: 2\n",
            "Total models: 20 (1 × 10 × 2)\n"
          ]
        }
      ],
      "source": [
        "# Calculate and print the dimensions of the stacking ensemble for clarity.\n",
        "n_cutoffs = len(TURN_CUTOFFS)\n",
        "n_base_models = 10 # Updated to 10 as LDA was added in build_base_models\n",
        "n_views = 2\n",
        "n_total_models = base_oof_all.shape[0]\n",
        "\n",
        "print(f\"Turn cutoffs: {TURN_CUTOFFS}\")\n",
        "print(f\"Base model types: {n_base_models}\")\n",
        "print(f\"Feature views: {n_views}\")\n",
        "print(f\"Total models: {n_total_models} ({n_cutoffs} \\u00d7 {n_base_models} \\u00d7 {n_views})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMTYMR9rvPMb",
        "outputId": "10399c84-051d-44c5-fc4e-184d808c43cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Turn cutoffs: [None]\n",
            "Base models per cutoff: 10\n",
            "Total models: 20\n"
          ]
        }
      ],
      "source": [
        "# Print summary information about the number of base models for the stacking ensemble.\n",
        "print(f\"Turn cutoffs: {TURN_CUTOFFS}\")\n",
        "print(f\"Base models per cutoff: {n_base_models}\") # This variable was defined as 10 now, reflecting the added LDA model\n",
        "print(f\"Total models: {n_total_models}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3Nv2gTivPMc",
        "outputId": "db692e95-cce2-45c7-de68-edaf351a0086"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Cutoff: full, View: timeline\n",
            "  Ridge      @ timeline    : 0.8201\n",
            "  QDA        @ timeline    : 0.7816\n",
            "  LogReg     @ timeline    : 0.8193\n",
            "  GB         @ timeline    : 0.8159\n",
            "  SVC        @ timeline    : 0.8106\n",
            "  RandomF    @ timeline    : 0.8109\n",
            "  AdaB       @ timeline    : 0.8134\n",
            "  ExtraT     @ timeline    : 0.8115\n",
            "  HistGB     @ timeline    : 0.8184\n",
            "  LDA        @ timeline    : 0.8205\n",
            "\n",
            "Cutoff: full, View: combat\n",
            "  Ridge      @ combat      : 0.8362\n",
            "  QDA        @ combat      : 0.7883\n",
            "  LogReg     @ combat      : 0.8324\n",
            "  GB         @ combat      : 0.8282\n",
            "  SVC        @ combat      : 0.8282\n",
            "  RandomF    @ combat      : 0.8142\n",
            "  AdaB       @ combat      : 0.8258\n",
            "  ExtraT     @ combat      : 0.8224\n",
            "  HistGB     @ combat      : 0.8309\n",
            "  LDA        @ combat      : 0.8334\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the accuracy of each individual base model's OOF predictions.\n",
        "cutoff_names = [f\"full\" if c is None else f\"t{c}\" for c in TURN_CUTOFFS]\n",
        "model_names_base = ['Ridge', 'QDA', 'LogReg', 'GB', 'SVC', 'RandomF', 'AdaB', 'ExtraT', 'HistGB', 'LDA'] # Added LDA\n",
        "view_names = ['timeline', 'combat']\n",
        "\n",
        "for cutoff_idx, cutoff_name in enumerate(cutoff_names):\n",
        "    for view_idx, view_name in enumerate(view_names):\n",
        "        print(f\"\\nCutoff: {cutoff_name}, View: {view_name}\")\n",
        "        for model_idx, model_name in enumerate(model_names_base):\n",
        "            # Calculate the global index for the current base model's predictions\n",
        "            global_idx = cutoff_idx * (n_base_models * n_views) + view_idx * n_base_models + model_idx\n",
        "\n",
        "            if global_idx < n_total_models:\n",
        "                preds = (base_oof_all[global_idx] >= 0.5).astype(int) # Convert probabilities to binary predictions\n",
        "                acc = accuracy_score(y_train, preds) # Calculate accuracy\n",
        "                print(f\"  {model_name:10} @ {view_name:12}: {acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpECzLf4_pA2",
        "outputId": "4ed44d96-e0a9-4a03-b29a-4b70fcba14b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average pairwise correlation: 0.888\n",
            "Min correlation: 0.719\n",
            "Max correlation: 1.000\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "# Calculate pairwise Spearman correlations between base models' OOF predictions.\n",
        "correlations = []\n",
        "for i in range(n_total_models):\n",
        "    for j in range(i+1, n_total_models):\n",
        "        corr, _ = spearmanr(base_oof_all[i], base_oof_all[j]) # Spearman correlation for rank agreement\n",
        "        correlations.append(corr)\n",
        "\n",
        "avg_corr = np.mean(correlations)\n",
        "min_corr = np.min(correlations)\n",
        "max_corr = np.max(correlations)\n",
        "\n",
        "print(f\"Average pairwise correlation: {avg_corr:.3f}\")\n",
        "print(f\"Min correlation: {min_corr:.3f}\")\n",
        "print(f\"Max correlation: {max_corr:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "id": "L5MfJDM9vPMd"
      },
      "outputs": [],
      "source": [
        "# Prepare the input matrix for the meta-model, concatenating base model predictions with a bias term.\n",
        "def meta_matrix(base_probs: np.ndarray) -> np.ndarray:\n",
        "    M = base_probs.T # Transpose to have (num_samples, num_models)\n",
        "    M = np.concatenate([M, np.ones((M.shape[0], 1))], axis=1) # Add a bias column of ones\n",
        "    return M\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "egT0HZt37Zf8"
      },
      "outputs": [],
      "source": [
        "# Create the meta-feature matrices for OOF and test predictions.\n",
        "X_meta_oof = meta_matrix(base_oof_all)\n",
        "X_meta_test = meta_matrix(base_test_all)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "id": "newtAG5UvPMf"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "from scipy.stats import spearmanr\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_predict, StratifiedKFold\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHlDfHYQvPMg",
        "outputId": "d47037d0-3780-4291-d972-c1878363a097"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "C=0.1: acc=0.8401, auc=0.9063, thr=0.488\n",
            "C=0.2: acc=0.8397, auc=0.9066, thr=0.484\n",
            "C=0.3: acc=0.8398, auc=0.9067, thr=0.542\n",
            "C=0.5: acc=0.8399, auc=0.9069, thr=0.497\n",
            "C=0.7: acc=0.8402, auc=0.9070, thr=0.493\n",
            "C=1.0: acc=0.8404, auc=0.9071, thr=0.508\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Train the meta-model (Logistic Regression) using cross-validation to get OOF predictions\n",
        "# and then fit on the full dataset to predict on the test data.\n",
        "for C in [0.1, 0.2, 0.3, 0.5, 0.7, 1.0]:\n",
        "    meta_model = LogisticRegression(\n",
        "        penalty='l2',\n",
        "        C=C,\n",
        "        fit_intercept=True,\n",
        "        solver='lbfgs',\n",
        "        max_iter=5000,\n",
        "        random_state=RANDOM_STATE\n",
        "    )\n",
        "\n",
        "    meta_oof_probs = cross_val_predict(\n",
        "        meta_model, X_meta_oof, y_train,\n",
        "        cv=10,\n",
        "        method='predict_proba'\n",
        "    )[:, 1]\n",
        "\n",
        "    thr = tune_threshold(meta_oof_probs, y_train, low=0.3, high=0.7, steps=401)\n",
        "    acc = accuracy_score(y_train, (meta_oof_probs >= thr).astype(int))\n",
        "    auc = roc_auc_score(y_train, meta_oof_probs)\n",
        "\n",
        "    print(f\"C={C}: acc={acc:.4f}, auc={auc:.4f}, thr={thr:.3f}\")\n",
        "\n",
        "\n",
        "\n",
        "# Train the final meta-model on the entire OOF meta-features\n",
        "meta_model.fit(X_meta_oof, y_train)\n",
        "# Get predictions on the test set using the trained meta-model\n",
        "meta_test_probs = meta_model.predict_proba(X_meta_test)[:, 1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "id": "rLjCOowavPMi"
      },
      "outputs": [],
      "source": [
        "# Calculate the average of all base models' OOF predictions and evaluate its accuracy and AUC.\n",
        "base_oof_avg = np.mean(base_oof_all, axis=0)\n",
        "base_acc = accuracy_score(y_train, (base_oof_avg >= 0.5).astype(int))\n",
        "base_auc = roc_auc_score(y_train, base_oof_avg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {
        "id": "4ywLXREZvPMk"
      },
      "outputs": [],
      "source": [
        "# Tune the optimal threshold for the meta-model's OOF predictions and evaluate its accuracy and AUC.\n",
        "global_thr = tune_threshold(meta_oof_probs, y_train, low=0.3, high=0.7, steps=401)\n",
        "meta_acc = accuracy_score(y_train, (meta_oof_probs >= global_thr).astype(int))\n",
        "meta_auc = roc_auc_score(y_train, meta_oof_probs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75dLwzQDvPMl",
        "outputId": "d3e74904-7bf3-4d2e-9ff5-ec1a86adf668"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Base OOF: 0.8334 (AUC: 0.9008)\n",
            "   Meta OOF @ 0.508: 0.8404 (AUC: 0.9071)\n",
            "   Improvement: +0.0070 (+0.8%)\n"
          ]
        }
      ],
      "source": [
        "# Print the performance comparison between the average base model and the meta-model.\n",
        "print(f\"   Base OOF: {base_acc:.4f} (AUC: {base_auc:.4f})\")\n",
        "print(f\"   Meta OOF @ {global_thr:.3f}: {meta_acc:.4f} (AUC: {meta_auc:.4f})\")\n",
        "print(f\"   Improvement: {meta_acc - base_acc:+.4f} ({(meta_acc/base_acc - 1)*100:+.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKNPyJhDvPMn",
        "outputId": "9613d821-b8fa-45e2-d13c-1680979dd9ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "0.8404\n"
          ]
        }
      ],
      "source": [
        "# Print the final meta-model OOF accuracy.\n",
        "print(f\"\\n{meta_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwQoIHugvPMo",
        "outputId": "97ece90a-0701-4084-c632-e67907746361"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Submission saved: /content/drive/MyDrive/challenge ds/data/test_res.csv\n",
            "Test predicted wins: 2471/5000 (49.4%)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Generate final predictions on the test set and save them to a CSV file.\n",
        "final_preds = (meta_test_probs >= global_thr).astype(int) # Apply the optimal threshold to test predictions\n",
        "\n",
        "submission = pd.DataFrame({\"battle_id\": test_ids, \"player_won\": final_preds})\n",
        "submission.to_csv(SUBMISSION_PATH, index=False)\n",
        "\n",
        "print(f\"\\nSubmission saved: {SUBMISSION_PATH}\")\n",
        "print(f\"Test predicted wins: {np.sum(final_preds)}/{len(final_preds)} ({np.mean(final_preds)*100:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSuOXOiPvPMp",
        "outputId": "3f31178b-d4e0-40c5-deb5-9493478db266"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average pairwise correlation: 0.888\n"
          ]
        }
      ],
      "source": [
        "# Calculate and print the average pairwise Spearman correlation among base model OOF predictions.\n",
        "correlations = []\n",
        "for i in range(n_total_models):\n",
        "    for j in range(i+1, n_total_models):\n",
        "        corr, _ = spearmanr(base_oof_all[i], base_oof_all[j])\n",
        "        correlations.append(corr)\n",
        "\n",
        "\n",
        "avg_corr = np.mean(correlations)\n",
        "print(f\"Average pairwise correlation: {avg_corr:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrERZ_gpvPMp",
        "outputId": "9e06aa30-983f-43b2-8e4b-9f1d69165262"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CV estimate of final test accuracy: 0.8378 ± 0.0048\n"
          ]
        }
      ],
      "source": [
        "# Perform cross-validated threshold tuning to estimate robust test accuracy.\n",
        "def oof_threshold_cv(probs, y, outer_folds=5, low=0.3, high=0.7, steps=201):\n",
        "    skf = StratifiedKFold(n_splits=outer_folds, shuffle=True, random_state=RANDOM_STATE)\n",
        "    accs = []\n",
        "\n",
        "    for tr, va in skf.split(probs.reshape(-1,1), y):\n",
        "        # Tune threshold on training fold and evaluate on validation fold\n",
        "        thr = tune_threshold(probs[tr], y[tr], low=low, high=high, steps=steps)\n",
        "        preds = (probs[va] >= thr).astype(int)\n",
        "        accs.append(accuracy_score(y[va], preds))\n",
        "\n",
        "    return np.mean(accs), np.std(accs)\n",
        "\n",
        "# Estimate the final test accuracy using cross-validated threshold tuning\n",
        "mean_cv_acc, std_cv_acc = oof_threshold_cv(meta_oof_probs, y_train)\n",
        "print(f\"CV estimate of final test accuracy: {mean_cv_acc:.4f} \\u00b1 {std_cv_acc:.4f}\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
